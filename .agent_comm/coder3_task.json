{
  "agent_id": "coder3",
  "task_id": "task_2",
  "files": [
    {
      "name": "requirements.txt",
      "purpose": "Python dependencies",
      "priority": "high"
    },
    {
      "name": "preprocessing.py",
      "purpose": "Image preprocessing utilities",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_stat.ML_2508.07876v1_Stochastic_dynamics_learning_with_state_space_syst",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on stat.ML_2508.07876v1_Stochastic-dynamics-learning-with-state-space-syst with content analysis. Detected project type: computer vision (confidence score: 6 matches).",
    "key_algorithms": [
      "Computing",
      "Deterministic",
      "Learned",
      "Unknown",
      "Hybrid",
      "General",
      "Machine",
      "Theoretical",
      "Successful",
      "Early"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "\n--- chunk_1.txt ---\nPDF: stat.ML_2508.07876v1_Stochastic-dynamics-learning-with-state-space-syst.pdf\nChunk: 1/2\n==================================================\n\n--- Page 1 ---\nStochastic dynamics learning with state-space systems\nJuan-Pablo Ortega\u2217Florian Rossmannek\u2217\nAbstract\nThis work advances the theoretical foundations of reservoir computing (RC) by providing a\nunified treatment of fading memory and the echo state property (ESP) in both deterministic\nand stochastic settings. We investigate state-space systems, a central model class in time series\nlearning, and establish that fading memory and solution stability hold generically \u2013 even in\nthe absence of the ESP \u2013 offering a robust explanation for the empirical success of RC models\nwithout strict contractivity conditions. In the stochastic case, we critically assess stochastic\necho states, proposing a novel distributional perspective rooted in attractor dynamics on the\nspace of probability distributions, which leads to a rich and coherent theory. Our results extend\nand generalize previous work on non-autonomous dynamical systems, offering new insights into\ncausality, stability, and memory in RC models. This lays the groundwork for reliable generative\nmodeling of temporal data in both deterministic and stochastic regimes.\n1. Introduction\nMany tasks arising in engineering and the sciences involve modeling time series data, that is, data points\nacquired at different points in time over a certain period. Such datasets could be gathered, for example, from\nobservations of a biological or physical system, measurements of patients\u2019 clinical data over time, or prices\nof stocks, among many others. This classical task has been treated comprehensively over the last decades,\ninitiated with the introduction of the modern computer. Lately, those classical methods have been replaced\nby machine learning methods in many applications [55,76].\nBroadly speaking, there are two main tasks one may be interested in with a given temporal dataset. The\nfirst one is prediction [36,56]: can we build a model that processes the temporal data and then predicts future\ndata points (a weather forecast [4,83], prognosis of a patient\u2019s condition [50], next day\u2019s stock price [25], next\nquarter\u2019s GDP of a country [8], etc. [51])? Forecasting requires a model to extract precise features of the\ntemporal dataset to understand how the evolution is governed. The second task is generation [28,58]: can\nour model generate new temporal data that behaves the same way as the original dataset without being an\nexact replica?\nData generation has various use cases. If we can generate reliable data, we can conduct simulations.\nThere, we are not interested in the specific trajectory seen in the original dataset. Instead, we intend\nto recover (physical or stochastic) features of the true underlying system. With those, we are then able\nto simulate, say, different weather and climate scenarios [4,83]. Scenario simulation enables us to infer\nfactors and confounders of the system. Concretely, tweaking variables of the model (representing actual\nvariables of the application) and conducting simulations with each constellation uncovers the influence of\nthose variables on the output. For example, running simulations on a patient\u2019s prognosis with varying\nparameters representing the prescribed dose of a medication yields insights on the implications of raising or\nlowering it on the probability of recovery [1].\nAnother use case of data generation is to enrich sparse datasets. Many numerical methods, most notably\nsome machine learning paradigms, are data-hungry: they require to be trained on large datasets to achieve\ngood performance. If data is available scarcely, one may use data generation to simulate new synthetic data\nthat inherits the same structure and features and can complement the original dataset. Subsequently, the\ndata-hungry methods can be run on the new dataset, comprised of the original and the synthetic data [80,82].\nConsider, for example, data gathered in a financial context. Although historical data may be available in\nabundance, market conditions, say, one hundred years ago do not reflect today\u2019s conditions. This renders\nold historical data less useful, and the data-subset of more recent, useful data becomes small. It has been\n\u2217Division of Mathematical Sciences, School of Physical and Mathematical Sciences, Nanyang Technological University,\nSingapore\n1arXiv:2508.07876v1  [stat.ML]  11 Aug 2025\n\n--- Page 2 ---\nOrtega and Rossmannek\nidentified as an important task in financial applications to enrich the resulting small dataset for training\nadvanced models [14,78].\nMathematically, many of these tasks fall in the category of (stochastic) filtering and control problems.\nAn early model used in that context was that of a Kalman filter [44]. The Kalman filter is a simple yet\npowerful model, which represents a special linear case of the more general family of models called state-space\nsystems [44,77]. These state-space systems form the basis for a range of different models, including recent\nmachine learning ones [35,41] for long sequence modeling. A machine learning paradigm that has proved\nitself successful in a range of applications is reservoir computing (RC) [39,40,60,61,81]. This paradigm is\nbased on a feedback loop, which takes a new input at each time step and transforms it together with its\nprevious state into a new state. Then, one extracts outputs by an additional transformation of the new\nstate. RC boasts several advantages over competing models. The reservoir performing the transformation\ncan be implemented by a physical system, outsourcing large parts of the computational costs from a classical\ncomputer. Various physical systems have been used successfully to this end, from ripples on the surface of\nwater [24], to optoelectronic circuits [3,52], to mechanical bodies [69]. The additional transformation that\nyields the output is usually taken to be linear. This makes it computationally cheap to train and to execute\nas well as makes the training of the model scale well as the size of the dataset and the dimensionality of\nthe problem grows [39,40]. These two features of RC become particularly desirable in face of the rapidly\ngrowing energy demands of other modern machine learning methods [11,42].\nIn addition to its practical benefits, RC enjoys a range of theoretical guarantees. In modern day machine\nlearning, theoretical guarantees have become ever more desirable due to the recent focus on reliability\nand explainability in artificial intelligence models. In the context of RC specifically, two main aspects\nbuild the foundation for the supporting theory. The first, like for classical neural network theory, are\nuniversal approximation results, which ensure that the learning task is indeed accurately solvable with an\nRC model [26,27,31,32]. The second aspect is a reservoir\u2019s ability to create reliable state responses to its\ninputs, commonly known as steady states or echo states [13,19,39]. The theory of echo states of a system\nis intimately linked to the model\u2019s behavior in how it handles its memory of the information it processed.\nReferred to as \u2018fading memory\u2019, the model is required to forget inputs from the distant past in order to create\nthose echo states [13]. Folklore has linked the fading of the memory to several related notions describing how\nboth past states\u2019 and past inputs\u2019 impact on the present state fade over time. A detailed discussion of the\nrelationship between those various notions can be found in [71]. In this work, we focus on echo states and\non fading memory.\nEven the notion of fading memory is not defined consistently throughout the literature [13,33,62]. All\ncommonly accepted definitions are phrased in terms of continuity of the echo states as a function of the\ninput sequence. But speaking of continuity requires fixing a topology on the space of sequences, which does\nnot admit a canonical choice. Different choices lead to different notions of fading memory [72]. In this work,\nwe pose a very general definition of fading memory, which captures most other definitions as special cases,\nincluding the original one introduced in the seminal work [13].\nOver the last decades, a lot of effort has been invested in finding necessary and sufficient conditions\nthat guarantee the existence of uniqueness of echo states \u2013 the so-called echo state property (ESP) \u2013 for\nvarious families of functions, such as echo state networks (ESNs) and state-affine systems [15,33,67,84].\nThese conditions typically boil down to contractivity requirements. If the model design of, for example,\nan ESN takes those findings into consideration, then the mathematical theory gives guarantees for fading\nmemory and, hence, on its successful implementation. In this paper, we show that successful implementation\ncan be expected even if the model design does notincorporate guarantees for the ESP. Namely, we prove\nthatstate-space systems enjoy fading memory and stability in the number of solutions for generic inputs ,\nindependent of the specific model design. This marks a breakthrough in the mathematical theory for the\nlearning of dynamics. Systems with more than one solution have previously been studied with the notion of\nan echo index introduced in [16] but the results therein also relied on contractivity assumptions. Our new\nresult that fading memory holds generically explains in particular that reservoirs do not generate intrinsic\nchaotic dynamics. The chaos exhibited by the reservoir system is solely injected through the chaos of the\ninput dynamics \u2013 a property that ought to be expected to enable successful learning since intrinsic chaos\nwould pose a serious obstruction.\nThe theory for RC is well-established in the context of deterministic learning tasks. However, many\napplications naturally generate stochastic data, which requires the theory to be adapted accordingly, in\nparticular for our understanding of generative models. Stochastic counterparts of established results have\nbegun to appear recently [73] but are far from complete. This work aims to largely close the gap between\nthe deterministic and the stochastic theory.\n2\n\n--- Page 3 ---\nStochastic dynamics learning with state-space systems\nOur findings will reveal many similarities but also striking differences between the deterministic and the\nstochastic settings. Deterministically, there is a natural way to define echo states as solutions to a state\nequation of the form xt=f(xt\u22121, ut), where fis the state map, xtare the states, and utare the inputs.\nThese solutions have been linked to the global attractor of the dynamical system encoding the dynamics\nof iterating the state map [62,64]. In the deterministic case, these two points of view lead to the same\nmathematical object.\nTodefinestochasticsolutions, oneconsidersstochasticprocessesthatsatisfy (Xt, Ut)t= (f(Xt\u22121, Ut), Ut)t.\nHere, it is crucial to consider the joint stochastic process since probabilistic dependence structures between\ninputs and states are one of the aspects that make the stochastic theory richer than the deterministic one.\nContrary to the deterministic case, we now face a choice: requiring the equality to hold in law or almost\nsurely. The former choice had been made in [66,73], whereas the latter choice is prevalent in time series\nanalysis [25]. In this work, we make a strong case that the latter choice is more natural. We do so by\ngeneralizing the link between solutions and the global attractor of the dynamical system that now encodes\nthe stochastic version of the state map (on the space of probability distributions). We find that the dynamic\npoint of view naturally leads to solutions defined by almost sure equality .\nThe newly established link between solutions and attractors in the stochastic context enable us to leverage\ntools from dynamical systems theory. If the system has the deterministic ESP and FMP, then stochastic\nsolutions are shown to be induced as push-forwards of the inputs under the deterministic solution map,\ngeneralizing a result of [73]. However, the stochastic theory becomes richer in that it admits solutions that\nelude a functional representation. We prove a stochastic generalization of the fundamental deterministic\nresult that the ESP implies fading memory on compact state spaces. Then, refining the study of stochastic\nsolutions, we discuss in detail the notion of causality, which captures the physical intuition that future\ninputs cannot directly affect present states. For such causal solutions, we prove that fading memory ensures\nstability in the number of solutions even in the absence of the ESP. Our proofs will be based on results\nabout abstract dynamical systems. From this, it will become clear exactly which are the key properties of\nstate-space systems that enable the interplay of the ESP and fading memory.\nWe treat the dynamics of state-space systems similarly as in [64,65], greatly expanding and generalizing\nseveral of the results established therein. In particular, [64] was one of the first works to study state-space\nsystems through the lens of abstract non-autonomous dynamical systems theory. The subsequent work [65]\nby the same authors as [64] extends their framework to include stochastic inputs. In this work, we promote\na concurrent treatment of the stochastic case, focusing on a distributional point of view that enables clean\nproofs relying on the same dynamical systems results. In this context, such a distributional point of view\nwas first considered in [66,73] and is improved upon in this work.\nFinally, the analysis of state-space systems in stochastic filtering and control tasks has previously led to\nthe consideration of fading memory [45,57]. We relate our new finding on generic fading memory back to\nthe classical filtering task as well as explore the meaning of causality of solutions in this context.\nThe exposition is structured as follows. In Sections 2 and 3, we present our findings on deterministic\nand stochastic state-space systems, respectively. The proofs of the results in these sections are postponed to\nSection 5. Section 4 develops results on abstract dynamical systems, on which the proofs in Section 5 are\nbased. Section 6 concludes. Appendix A contains additional technical lemmas, and Appendix B discusses\nlinks to classical stochastic filtering.\nConventions\nWe denote by Zthe set of integers, by Nthe set of strictly positive integers, N0=N\u222a {0}, andZ\u2212=Z\\N.\nWe understand the size #Aof a set Aas a number in N0\u222a {\u221e}, that is, \u2018 #A\u2019 does not distinguish between\ncountable infinite and uncountable infinite cardinality. Given any Cartesian product A\u00d7B, the letter \u03c0\nwith the subscript A(orB) denotes the natural projection \u03c0A:A\u00d7B\u2192A(or\u03c0B:A\u00d7B\u2192B). Subsets\nof topological spaces and products of topological spaces are endowed with the subspace topology and the\nproduct topology, respectively, unless explicitly stated otherwise. All topological spaces are endowed with\ntheir Borel sigma-algebra. For any Hausdorff space X, we denote by P(X)the set of all Radon probability\nmeasures. In particular, if Xis a Radon space (e.g. Polish), then P(X)coincides with the set of all Borel\nprobability measures. The following table provides an overview of our notation.\n3\n\n--- Page 4 ---\nOrtega and Rossmannek\nfunction letter domain and codomain\nstate map f X \u00d7 U \u2192 X\nreadout h X \u2192 Y\nextended state map F X\u2212\u00d7 U\u2212\u2192 XZ\u2212\u00d7 UZ\u2212\nextended readout H X\u2212\u00d7 U\u2212\u2192 Y\u2212\u00d7 U\u2212\ndynamical system \u03c6 X\u2212\u00d7 U\u2192 X\u2212\u00d7 U\nright-shift operator T U\u2212\n\u25c0\u2283orX\u2212\n\u25c0\u2283orX\u2212\u00d7 U\u2212\n\u25c0\u2283orX\u2212\u00d7 U \u25c0\u2283\nleft-shift operator \u03c3 U\u2192 U\ntruncation \u03c4 U\u2192 U\u2212orX\u2212\u00d7 U\u2192 X\u2212\u00d7 U\u2212\nright-inv. of truncation j+X\u2212\u00d7 U\u2212\u2192 X\u2212\u00d7 U\ninclusion \u03b9 X\u2212\u00d7 U\u2212\u2192 XZ\u2212\u00d7 UZ\u2212\nprojection \u03c0X\u2212 X\u2212\u00d7 U\u2212\u2192 X\u2212orX\u2212\u00d7 U\u2192 X\u2212\nprojection \u03c0U\u2212 X\u2212\u00d7 U\u2212\u2192 U\u2212orY\u2212\u00d7 U\u2212\u2192 U\u2212\nprojection \u03c0U X\u2212\u00d7 U\u2192 U\ndeterministic attractor S\u03c6 U\u21922X\u2212\u00d7U\ndeterministic solutions SdetU\u2212\u21922X\u2212\u00d7U\u2212\ndeterministic outputs OdetU\u2212\u21922Y\u2212\u00d7U\u2212\nstoch. dynamical system \u03c6\u2217 Pstate\u2192 P state\nstoch. left-shift operator \u03c3\u2217 Pin\u2192 P in\nstoch. right-shift operator T\u2217 P\u2212\nin\u25c0\u2283orP\u2212\nstate\u25c0\u2283orPstate\u25c0\u2283orP(X\u2212\u00d7 U)\u25c0\u2283\nstoch. projection (\u03c0U\u2212)\u2217 P\u2212\nstate\u2192 P\u2212\ninorP\u2212\nout\u2192 P\u2212\nin\nstoch. projection (\u03c0U)\u2217 Pstate\u2192 P in\nstoch. attractor S\u03c6\u2217 Pin\u21922Pstate\n(causal) stoch. solutions Sstoch,Sc,stochP\u2212\nin\u21922P\u2212\nstate\n(causal) stoch. outputs Ostoch,Oc,stochP\u2212\nin\u21922P\u2212\nout\n2. Deterministic state-space systems\nThroughout this section, let U,X, andYbe Hausdorff spaces. Sequences are denoted as underlined letters,\ne.g.u= (ut)t\u2208Z\u2212\u2208 UZ\u2212. We denote the right-shift operator (ut)t7\u2192(ut\u22121)ton all left- and bi-infinite\nsequence spaces by the letter Tand the left-shift operator (ut)t7\u2192(ut+1)ton bi-infinite sequence spaces by\n\u03c3. Fix backwards shift-invariant subsets U\u2212\u2286 UZ\u2212,X\u2212\u2286 XZ\u2212, andY\u2212\u2286 YZ\u2212, that is, T\u22121(U\u2212) =U\u2212\nand likewise for X\u2212andY\u2212, and fix a shift-invariant subset U\u2286 UZthat is mapped surjectively onto U\u2212by\nthe truncation, that is, \u03c3(U) =Uand\u03c4(U) =U\u2212, where \u03c4:U\u2192 U\u2212,u7\u2192(ut)t\u2208Z\u2212. We will use the same\nletter \u03c4to also denote the truncation X\u2212\u00d7 U\u2192 X\u2212\u00d7 U\u2212,(x, u)7\u2192(x, \u03c4(u))on the domain X\u2212\u00d7 U. The\ntopologies on these sequence spaces are subject to the following conditions.\nAssumption 2.1. The sets U\u2212,X\u2212,Y\u2212, andUare endowed with topologies that are at least as fine as the\nproduct topologies and that satisfy the following properties.\n(i)The right-shift operators on U\u2212,X\u2212, and Y\u2212are continuous, and the left-shift operator on Uis a\nhomeomorphism.\n(ii) The truncation map U\u2192 U\u2212is continuous and admits a continuous right-inverse U\u2212\u2192 U.\n(iii)The concatenation map X\u2212\u00d7 X \u2192 X\u2212,(x, x)7\u2192(. . . , x\u22121, x0, x)is continuous, as is the analogous\nmapU\u2212\u00d7 U \u2192 U\u2212.\n(iv) For any u, u\u2032\u2208 U\u2212, the sequence un:= (. . . , u\u2032\n\u22121, u\u2032\n0, u\u2212n, . . . , u0)\u2208 U\u2212converges to uasn\u2192 \u221e.\n4\n\n--- Page 5 ---\nStochastic dynamics learning with state-space systems\nExample 2.2. Admissible topologies for U\u2212andUinclude the product topology and the topologies induced\nby weighted \u2113p-norms, p\u2208[1,\u221e]. The same topologies are admissible for X\u2212as well as the topologies\ninduced by unweighted \u2113p-norms, p\u2208[1,\u221e]. The topology induced by a weighted \u2113\u221e-norm is the one used\nby Boyd and Chua in their seminal work on fading memory [13]. For further details on these particular\nchoices of topology, we refer to [33,72].\nWe remark that all sequence spaces are guaranteed to be Hausdorff since the base spaces are Hausdorff\nand the topologies on the sequence spaces are, by hypothesis, at least as fine as the product topologies.\n2.1 Dynamics on sequence spaces\nConsider a continuous state map f:X \u00d7 U \u2192 X . Asolution of the state equation is a pair of sequences\n(x, u)\u2208 X\u2212\u00d7 U\u2212that satisfies xt=f(xt\u22121, ut)for all t\u2208Z\u2212. In this case, we also call xasolution for\nthe input u. Denote the set of all solutions by (\u2018det\u2019 for deterministic)\nSdet:={(x, u)\u2208 X\u2212\u00d7 U\u2212:xt=f(xt\u22121, ut)for all t\u2208Z\u2212}.\nAlso consider the dynamical system \u03c6:X\u2212\u00d7 U\u2192 X\u2212\u00d7 Ugiven by \u03c6(x, u) = (( x, f(x0, u1)), \u03c3(u)), which is\nan extension to the input sequence space of the reservoir flow considered in [33]. Note that \u03c6is continuous\nby Assumption 2.1. In an abstract dynamical systems sense, a solution of \u03c6is a point through which a\nbi-infinite orbit of \u03c6passes [48]. Since the right-shift operator Tis a left-inverse of \u03c6, the set S\u03c6of solutions\nof\u03c6can be characterized as follows;\nS\u03c6={(x, u)\u2208 X\u2212\u00d7 U:\u03c6n(Tn(x, u)) = ( x, u)for all n\u2208N}.\nIn fact, S\u03c6=T\nn\u2208N0\u03c6n(X\u2212\u00d7 U)is the global attractor of \u03c6. We will review attractors and solutions of\nabstract dynamical systems in more detail later in Section 4. For the state equation, we make the following\nimportant observation, which tells us that to understand the solutions of the state equation we may study\nthe attractor of \u03c6. This paves the way to leveraging tools from autonomous dynamical systems theory.\nProposition 2.3. The set of solutions of the state equation satisfies Sdet=\u03c4(S\u03c6)andS\u03c6=\u03c4\u22121(Sdet).\nThe dynamical system \u03c6processes bi-infinite input sequences in its second argument. The reason\nto work with full bi-infinite input sequences is that we do not prescribe a rule by which the inputs are\ngenerated. Allowing \u03c6to access the next input u1in the input sequence ucovers the general case of arbitrary\ninputs. Let us spell out an important special case. More precisely, suppose (only for the remainder of\nthis subsection) that the inputs are generated by an invertible dynamical system \u03d5:M \u2192 M , masked by\nan observation function \u03c9:M \u2192 U , which is a customary setup in the literature [9,29,30]. Then, one\nconsiders the autonomous dynamical system \u03c8:X \u00d7 M \u2192 X \u00d7 M given by \u03c8(x, p) = (f(x, \u03c9(\u03d5(p))), \u03d5(p)),\nwhich evolves on the base spaces instead of the sequence spaces. To recover this special case, we take\nX\u2212=XZ\u2212to be the entire sequence space and U=\u03c9(M)to be the image of the map \u03c9:M \u2192 UZ,\np7\u2192(\u03c9(\u03d5t(p)))t\u2208Z. With these choices, we ensure that S\u03c8={(x0, p):(x,\u03c9(p))\u2208 S\u03c6}, where S\u03c8is the\nset of solutions of \u03c8, which we recall are points through which a bi-infinite orbit of \u03c8passes. Conversely,\nS\u03c6={(x, u):(xt, pt)\u2208 S\u03c8for some pt\u2208\u03c9\u22121(\u03c3t(u))for all t\u2208Z\u2212}. Furthermore, if \u03b6:U\u2192 X\u2212is a\ngeneralized synchronization (GS, see Sections 4.1.3 and 4.1.4 and [2,49,59]) for \u03c6, meaning that we have the\nsemi-conjugacy \u03c6\u25e6(\u03b6\u00d7idU) = (\u03b6\u00d7idU)\u25e6\u03c3, then \u03b60:M \u2192 X,p7\u2192(\u03b6\u25e6\u03c9(p))0is a GS for \u03c8, meaning that\nwe have the semi-conjugacy \u03c8\u25e6(\u03b60\u00d7idM) = (\u03b60\u00d7idM)\u25e6\u03d5. Conversely, if \u03b60:M \u2192 X is a GS for \u03c8, then\nthe set \u03b60(\u03c9\u22121(u))is a singleton for all u\u2208 Ueven if \u03c9is not injective, and a GS for \u03c6can be recovered by\n\u03b6:U\u2192 X\u2212,u7\u2192(\u03b60(\u03c9\u22121(\u03c3t(u))))t\u2208Z\u2212. We point out that \u03b6synchronizes the state-space dynamics with the\n\u2018dynamics\u2019 of the observations, whereas \u03b60synchronizes with the true dynamics of the underlying system \u03d5.\nIn particular, notice that \u03b6is injective if \u03b60is injective. In this case, if \u03b70:\u03b60(M)\u2192 Mis the left-inverse of\n\u03b60, then \u03b7:\u03b6(U)\u2192 Ugiven by \u03b7(x) = (\u03c9\u25e6\u03b70(xt))t\u2208Zis the left-inverse of \u03b6. However, injectivity of \u03b6does\nnot necessarily transfer to \u03b60. This injectivity is commonly regarded as \u2018learnability\u2019 since the existence of\nthe inverse means that we can learn a readout that returns the next observation from the current state of\nthe state-space system.\nIf the dynamical system \u03d5generating the inputs satisfies the hypothesis of Takens\u2019 embedding theorem [79]\nand\u03c9is a generic observation function, then the map \u03c9:MZ\u2192 U,p7\u2192(\u03c9(pt))t\u2208Zis injective on the set\nM:={(\u03d5t(p))t\u2208Z:p\u2208 M}. In this case, we can regard \u03c6as a bundle map over the left-shift on Mon\nthe bundle \u03c0\u2032:=\u03c9\u22121\u25e6\u03c0U:X\u2212\u00d7 U\u2192 M(see Section 4.1.4), and a GS for \u03c6synchronizes the state-space\ndynamics also with the true dynamics of the underlying system instead of the observations. The various\nmaps and semi-conjugacies are summarized in the commutative diagrams in Figure 1.\n5\n\n--- Page 6 ---\nOrtega and Rossmannek\n\u03b6(U) \u03b6(U)\nS\u03c6 S\u03c6\nU U\nM M\u03b7 \u03b7\u03c0X\u2212\n\u03c0U\u03c6\n\u03c0U\u03c0X\u2212\n\u03b6\u00d7idU\n\u03c3\u03b6\u00d7idU\n\u03c9\n\u03d5\u03c9M M\nS\u03c8 S\u03c8\n\u03b60(M) \u03b60(M)\u03b60\u00d7idM\u03d5\n\u03b60\u00d7idM\n\u03c0X\u03c0M\n\u03c8\u03c0M\n\u03c0X\u03b70 \u03b70\u03b6\u2032(U) \u03b6\u2032(U)\nS\u03c6 S\u03c6\nM M\nM M\u03b7\u2032\u03b7\u2032\u03c0X\u2212\n\u03c0\u2032\u03c6\n\u03c0\u2032\u03c0X\u2212\n\u03b6\u2032\u00d7\u03c9\n\u03c3\u03b6\u2032\u00d7\u03c9\nOrb\u03d5\n\u03d5Orb\u03d5\nFigure 1: Commutative diagrams depicting the relations in Section 2.1. The dashed horizontal arrows\ndepict maps induced by \u03b7,\u03b70, and \u03b7\u2032by a diagram chase. The dashed vertical arrows indicate that the\nmiddle diagram could be added at the bottom of either the left or right diagram to create larger commutative\ndiagrams.\n2.2 Echo states\nIn addition to the state map f, consider a readout h:X \u2192 Yfor which x7\u2192(h(xt))t\u2208Z\u2212poses a well-defined\nand continuous map X\u2212\u2192 Y\u2212. Anoutputof the state-space system is a pair of sequences (y, u)\u2208 Y\u2212\u00d7U\u2212\nfor which there exists a solution x\u2208 X\u2212for the input uthat satisfies h(xt) =ytfor all t\u2208Z\u2212. Naturally,\nwe also call yan output for the input u.\nSolutions of state-space systems can be seen as special cases of outputs of state-space systems simply\nby taking the identity as readout. However, it is instructive to treat solutions and outputs separately to\nemphasize the connection of solutions with the attractor of the dynamical system exhibited in the previous\nsection.\nDefinition 2.4. We say that the state-space system has the echo state property (ESP) if there exists\na unique solution x\u2208 X\u2212for any given input u\u2208 U\u2212; and we say that it has the ESP in the outputs if\nthere exists a unique output y\u2208 Y\u2212for any given input u\u2208 U\u2212.\nClearly, if the state-space system has the ESP, then it also has the ESP in the outputs. We had introduced\nthe set Sdetof solutions in the previous section. Analogously, denote the set of all outputs by\nOdet:={H(x, u)\u2208 Y\u2212\u00d7 U\u2212: (x, u)\u2208 Sdet},\nwhere H:X\u2212\u00d7 U\u2212\u2192 Y\u2212\u00d7 U\u2212denotes the extended readout (x, u)7\u2192((h(xt))t\u2208Z\u2212, u). In the presence\nof the ESP, we obtain well-defined maps U\u2212\u2192 X\u2212andU\u2212\u2192 Y\u2212that associate to any given input\nits unique solution and output, respectively. Without the ESP, we instead consider the set-valued map\nu7\u2192 {(x, u):(x, u)\u2208 Sdet}. By a slight abuse of notation, we denote this set-valued map by u7\u2192 Sdet(u)\nsince it is exactly the fiber map u7\u2192 Sdet\u2229\u03c0\u22121\nU\u2212(u)of the set Sdet. We similarly regard Odetas a set-valued\nfiber map.\nAs motivated in the introduction, the ESP is closely linked to the notion of fading memory [13,72], which\nwe introduce next. The concept of fading memory is, in fact, formalized by Assumption 2.1.(iv). Indeed,\nfading memory is typically realized as a property of the topology on the domain of a filter, with respect\nto which continuity is required to hold. Here, we possibly work with set-valued maps, with which we can\nstate the most general definition, following [62]. The notion of hemi-continuity generalizes the concept of\ncontinuity to set-valued maps. A set-valued map S:V \u2192 2W, where VandWare topological spaces, is\nhemi-continuous at a point v\u2208 Vif for any two open sets W, W\u2032\u2286 Wwith W\u2229S(v)\u0338=\u2205andS(v)\u2286W\u2032\nthere exists an open neighborhood V\u2286 Vofvsuch that W\u2229S(v\u2032)\u0338=\u2205andS(v\u2032)\u2286W\u2032for all v\u2032\u2208V.\nDefinition 2.5. We say that the state-space system has the fading memory property (FMP) at a given\ninput u\u2208 U\u2212ifSdetis hemi-continuous at u. We denote the set of all points at which the state-space system\n6\n\n--- Page 7 ---\nStochastic dynamics learning with state-space systems\nhas the FMP by D(Sdet). We say that the state-space system has the FMP if D(Sdet) =U\u2212. TheFMP in\nthe outputs and the set D(Odet)are defined analogously.\nThat the ESP implies the FMP has been claimed and proved before [39,62]. However, [39] crucially\nassumed compactness not just of the state space but also of the input space. The most general result thus\nfar has been [62, Corollary 3.3]. Our next result is a slight generalization thereof.\nTheorem 2.6. Suppose X\u2212is compact. If the state-space system has the ESP (in the outputs), then it has\nthe FMP (in the outputs).\nExample 2.7. In general, the assumption that X\u2212be compact cannot be dropped. Let U\u2212=\u2113\u221e(Rd)and\nX\u2212=\u2113\u221e(Rn)with the product topologies. Consider a matrix A\u2208Rn\u00d7nwith a spectral radius strictly\nsmaller than 1 and a matrix B\u2208Rn\u00d7d. The linear state-space system driven by f(x, u) =Ax+Buhas\nthe ESP; indeed, \u03c0X\u2212(Sdet(u)) ={(P\ns\u22640A\u2212sBut+s)t\u22640}. However, since U\u2212andX\u2212are endowed with\nthe product topologies, the map \u03c0X\u2212\u25e6 Sdet:U\u2212\u2192 X\u2212is known to be continuous if and only if Ais\nnilpotent [72, Proposition 11].\nNoteworthy in Theorem 2.6 is that neither compactness of the state sequence space nor the ESP depend\non the choice of topology on the input sequence space; yet, together they imply the FMP, which explicitly\ndepends on that choice. In particular, Theorem 2.6 holds for the coarsest topology that we admit for defining\nthe FMP, namely the product topology. We take note of this consequence in the corollary below.\nCorollary 2.8. LetA:U\u2212\u2192 Y\u2212be a function, and suppose there exist a continuous state map and a\ncontinuous readout with the ESP in the outputs such that A(u)is the output for the input u, for all u\u2208 U\u2212.\nIf the state sequence space on which the state-space system evolves can be taken compact, then Ais continuous\nwith respect to the product topology on U\u2212.\nIt is known that linear time-invariant filters \u2113\u221e(R)\u2192\u2113\u221e(R)that are continuous with respect to a\nweighted sup-norm can be realized by a linear state-space system with the ESP in the outputs [34]. However,\nthe state space on which this state-space system lives is a potentially complicated object. Conditions that\nguarantee compactness in this context are unknown, as are generalizations to the non-linear case.\n2.3 Generic fading memory\nEmbodying the notion of continuity of set-valued maps, we can show that the FMP implies stability of the\nnumber of solutions as a function of the input.\nProposition 2.9. The map D(Sdet)\u2192N0\u222a {\u221e},u7\u2192#Sdet(u)is constant. Furthermore, if we denote\nthis constant by M, then #Sdet(u)\u2265Mfor any u\u2208 U\u2212. In particular, if the state-space system has the\nFMP, then #Sdet(u)is constant on U\u2212, and if it has the FMP and #Sdet(u) = 1for some u\u2208 U\u2212, then it\nhas the ESP.\nWe had pointed out that Theorem 2.6 holds for any choice of topology on U\u2212admissible under Assump-\ntion 2.1 so that, in particular, we could have chosen the product topology to obtain the strongest notion of\nfading memory in Theorem 2.6. The same applies to Proposition 2.9, except that this time the FMP acts\nas a hypothesis. In this case, we could have chosen an arbitrarily fine topology on U\u2212as long as it satisfies\nAssumption 2.1.\nProposition 2.9 generalizes a result of [62,63]. In those works, it is assumed that #Sdet(u) = 1for some\nu\u2208 U\u2212, in which case #Sdet(u)being constant becomes the ESP. More importantly, Proposition 2.9 poses\nno additional assumptions on the state or input space such as compactness. To extend this result to the\noutputs, we adapt a few notions from control theory to suit our context [34,77].\nDefinition 2.10. Two states x, x\u2032\u2208 Xare called simultaneously reachable if there exist two solutions\n(x, u),(x\u2032, u)\u2208 Sdetwith x0=xandx\u2032\n0=x\u2032. Two states x, x\u2032\u2208 Xare called distinguishable if there\nexists some u\u2208 Usuch that h(f(x, u))\u0338=h(f(x\u2032, u)). We say that the state-space system distinguishes\nreachable states1ifh(x)\u0338=h(x\u2032)for any two distinguishable simultaneously reachable states x, x\u2032\u2208 X.\n1Our notionof distinguishing reachablestates is closelyrelated but notidentical to thecontrol-theoretic notion ofobservability.\nThe difference is, firstly, that our notion considers only simultaneously reachable states, that is, states that can be reached with\nthe same input/control sequence; and, secondly, our notion asks two such states to be instantaneously distinguishable whenever\nthey are distinguishable in time 1, in the language of [77].\n7\n\n--- Page 8 ---\nOrtega and Rossmannek\nIt is clear that a state-space system with the ESP distinguishes reachable states since then there are no\ndistinct simultaneously reachable states.\nRemark 2.11. The state-space system distinguishes reachable states if and only if h(x0) =h(x\u2032\n0)for any\n(x, u),(x\u2032, u)\u2208 Sdetwith h(x\u22121) =h(x\u2032\n\u22121).\nExample 2.12. Consider the linear state-space system f(x, u) =Ax+Buandh(x) =Wxwith matrices\nA\u2208Rn\u00d7n,B\u2208Rn\u00d7d, and W\u2208Rm\u00d7n. Then, the system distinguishes reachable states if Amaps the kernel\nofWinto the kernel of W.\nProposition 2.13. If the state-space system distinguishes reachable states, then the analog statement of\nProposition 2.9 holds with Odetin place of Sdet.\nExample 2.14. In general, the assumption that the state-space system distinguishes reachable states cannot\nbe dropped. Consider the linear state-space system from Example 2.12. Extend the system to the one-point\ncompactification X:=Rn\u222a {\u221e} \u223c=Snby setting f\u2032=fonRn\u00d7Rdandf\u2032=\u221eon{\u221e} \u00d7 Rd. This\nextension f\u2032is continuous if the matrix Ais injective. As state sequence space, take X\u2212=XZ\u2212\u2283\u2113\u221e(Rn).\nThe extended system has exactly two solutions for any input u\u2208 U\u2212, namely the original solution and the\nsequence \u221ethat is constantly \u221e. Suppose Bis injective so that (0, u)/\u2208 Sdet(u)for any u\u0338=0. Take a\ncontinuous readout h:X \u2192Rthat maps the poles and only the poles to zero, that is, h\u22121(0) ={0,\u221e}. Then,\nOdet\nf\u2032(0) ={(0, u)}and#Odet\nf\u2032(u) = 2for any u\u0338=0. The new system does not distinguish reachable states\nbecause 0and\u221eare two distinguishable simultaneously reachable states. The state-space system has the\nFMP in the outputs if U\u2212is endowed with the topology induced by a weighted supremums norm [72, Theorem\n10].\nThe next result says that if the underlying spaces exhibit more regularity, then the FMP holds generically\nand bifurcations of solutions (and outputs) can occur only within a meager set. We recall that a subset of a\ntopological space is residual if it can be written as a countable intersection of subsets each of which has a\ndense interior; a subset is meager if its complement is residual; and a property holds \u2018generically\u2019 if it holds\non a residual subset.\nTheorem 2.15. Suppose that U\u2212,X\u2212, and Y\u2212are Polish and that X\u2212is compact. Then, D(Sdet)and\nD(Odet)contain a residual set. In particular, #Sdet(u)is generically constant, and so is #Odet(u)if the\nstate-space system distinguishes reachable states.\nWe have seen in Proposition 2.9 that the number of solutions is constant in the input if the state-space\nsystem has the FMP. We conclude this section with the conjecture that the converse also holds.\nConjecture 2.16. If#Sdet(u)is constant on U\u2212and this constant is finite, then the state-space system\nhas the FMP.\n2.4 On a prominent example\nThe dynamical system induced by the state map f(x, u) =ux/(1 +|x|)had been introduced in [46] to\nillustrate subtleties of pullback and forward attraction in non-autonomous systems. This example had been\nrevisited in the context of echo states both in [63] and, with an additional hyperbolic tangent transformation,\nin [16]. Suppose we feed the state map with the input sequence uthat is constantly u\u2212\u2208(0,1)in negative\ntime and then switches to being constantly u+\u2208(1,2)in positive time. The only solution for the input uis\nthe state sequence x=0that is constantly zero. Since we have seen that solutions form the global attractor\nof the dynamical system, it is therefore tempting to think that trajectories of the state-space system are\nattracted to the constant zero sequence. However, this is not the case as noted in [16,46]; even more so, the\nconstant zero sequence is a repeller. It had been argued in [16] that the issue is due to pullback versus forward\nattraction. On the contrary, the results in this work reveal that we cannot expect trajectories to converge\nto the solution for the input u. Indeed, guided by the idea of fading memory (which we proved to hold\ngenerically), after a while the dynamical system can hardly distinguish between the input sequence uand\nthe input sequence u+that is constantly u+. For the input sequence u+, there are three solutions 0, x+, x\u2212,\nnamely the state sequences that are constantly one of the fixed points 0, x+, x\u2212of the map x7\u2192u+x/(1+|x|).\nThus, trajectories of the state-space system with input uare expected not to be attracted to the single\nsequence 0but instead to the set {0, x+, x\u2212}, which is indeed the behavior that had been observed in the\nnumerical illustrations in [16,46]. In particular, the (fiber of the) global pullback attractor {0, x+, x\u2212}acts\n8\n\n--- Page 9 ---\nStochastic dynamics learning with state-space systems\nas a forward attractor as well in this case. The relationship between echo states and forward attraction has\nbeen investigated in more detail in [71]. We have proved in this work that the number of solutions is constant\non a residual set. In particular, in the example above, the input sequence is taken from a meager set and\nrepresents an unlikely scenario for actual deployment of the model. At the same time, this means that fading\nmemory does not hold at the input u. However, what breaks down at uis lower hemi-continuity whereas\nupper hemi-continuity still provably holds \u2013 and it is upper hemi-continuity that warrants the intuition for\nconverging to the solution set of u+as opposed to the solution of u.\n3. Stochastic state-space systems\nWe adopt the notation from the previous section, continue to pose Assumption 2.1, and assume addition-\nally that U\u2212,X\u2212,Y\u2212, and Uare completely regular. Let P\u2212\nstate\u2286P(X\u2212\u00d7 U\u2212)andP\u2212\nin\u2286P(U\u2212)be\nbackwards shift-invariant, that is, (T\u2217)\u22121(P\u2212\nstate) =P\u2212\nstateand(T\u2217)\u22121(P\u2212\nin) =P\u2212\nin, respectively, and satisfy\n(\u03c0U\u2212)\u2217(P\u2212\nstate) =P\u2212\nin. Also, let P\u2212\nout:= (\u03c0U\u2212)\u22121\n\u2217(P\u2212\nin)\u2286P(Y\u2212\u00d7 U\u2212). In the deterministic case, we studied\nthe global attractor of the dynamical system \u03c6. In the stochastic case, we replace \u03c6by its push-forward,\nwhose domain we pick as follows. Let Pstate:= (\u03c4\u2217)\u22121(P\u2212\nstate)\u2286P(X\u2212\u00d7 U)andPin:= (\u03c4\u2217)\u22121(P\u2212\nin)\u2286P(U).\nWe point out that PstateandPinare backwards shift-invariant and satisfy (\u03c0U)\u2217(Pstate)\u2286 P in. The reverse\ninclusion Pin\u2286(\u03c0U)\u2217(Pstate)holds if Uis Polish (this follows from Lemma 5.9 further below).\nAssumption 3.1. The sets P\u2212\nin,Pin,P\u2212\nstate,Pstate, and P\u2212\noutare endowed with topologies that are at\nleast as fine as the weak topologies and such that the following push-forward maps are all continuous:\n(\u03c0U\u2212)\u2217:P\u2212\nstate\u2192 P\u2212\nin,(\u03c0U)\u2217:Pstate\u2192 P in,T\u2217:Pstate\u2192 P state,\u03c3\u2217:Pin\u2192 P in,\u03c6\u2217:Pstate\u2192 P state,\nH\u2217:P\u2212\nstate\u2192 P\u2212\nout\nExample 3.2. The weak topologies on P\u2212\nin,Pin,P\u2212\nstate,Pstate, andP\u2212\noutare admissible. Indeed, push-forward\nmaps induced by continuous maps are continuous on the spaces of Radon probability measures with respect\nto the weak topologies if the underlying spaces are completely regular [10, Proposition 7.2.2 and Theorem\n8.4.1]. Furthermore, we point out that complete regularity of the underlying spaces makes the weak topologies\nHausdorff [10, Theorem 7.10.6 and Corollary 8.2.5].\n3.1 Stochastic echo states\nGiven an element \u00b5\u2208P(X\u2212\u00d7 U\u2212), a realization of \u00b5is a pair (X, U)of anX\u2212-valued and a U\u2212-valued\nrandom variable2that are defined on some common probability space and whose joint law is \u00b5. We call\n\u00b5\u2208 P\u2212\nstateastochastic solution of the state-space system if some realization (X, U)of\u00b5is a solution of\nthe state equation almost surely, that is, Xt=f(Xt\u22121, Ut)holds almost surely for all t\u2208Z\u2212. As such, a\nstochastic solution not only captures the law of the states but also encodes the input law in its U\u2212-marginal\nand the dependence structure between the states and the inputs. Given an input law \u039e\u2208 P\u2212\nin, an element\n\u00b5\u2208 Sstochis called a stochastic solution for the input law \u039eif(\u03c0U\u2212)\u2217\u00b5= \u039e. The set of all stochastic\nsolutions will be denoted Sstoch\u2286 P\u2212\nstate.\nIn the previous section, we observed that deterministic solutions are (truncations of) elements of the global\nattractor of the dynamical system \u03c6. In the stochastic case, the push-forward \u03c6\u2217acts as an autonomous\ndynamical system on Pstate. The global attractor of \u03c6\u2217is\nS\u03c6\u2217=\\\nn\u2208N\u03c6n\n\u2217(Pstate) ={\u00b5\u2208 Pstate:\u03c6n\n\u2217Tn\n\u2217\u00b5=\u00b5for all n\u2208N}.\nAlthough stochastic solutions are defined through almost sure equality of the state equation and \u03c6\u2217only sees\nlaws, we recover the analog of the deterministic result from Proposition 2.3.\nProposition 3.3. The set of stochastic solutions satisfies Sstoch=\u03c4\u2217(S\u03c6\u2217). Furthermore, for any \u00b5\u2208 P\u2212\nstate,\nthe following are equivalent.\n(i)Some realization of \u00b5is a solution of the state equation almost surely.\n(ii)Every realization of \u00b5is a solution of the state equation almost surely.\n(iii)The support of \u00b5is contained in Sdet.\n2AnX\u2212-valued random variable defines a stochastic process on Xbut not every stochastic process on Xdefines a random\nvariable on X\u2212if the topology on X\u2212is finer than the product topology.\n9\n\n--- Page 10 ---\nOrtega and Rossmannek\nIn the introduction, we discussed that the stochastic context requires making a choice when defining\nsolutions of the state-space system: almost sure equality of the state equation versus equality in law. More\nformally, consider the continuous map F:X\u2212\u00d7 U\u2212\u2192 XZ\u2212\u00d7 UZ\u2212,(x, u)7\u2192(f(xt\u22121, ut), ut)t\u2208Z\u2212and the\ninclusion \u03b9:X\u2212\u00d7 U\u2212\u2192 XZ\u2212\u00d7 UZ\u2212. In the deterministic context, it is clear that solutions to the state\nequation are precisely the fixed points of F. We call \u00b5\u2208 P\u2212\nstateastochastic fixed-point solution of the\nstate equation if F\u2217\u00b5=\u03b9\u2217\u00b5. Since almost sure equality of the state equation implies equality in law, it is clear\nthat any stochastic solution of the state-space system is also a stochastic fixed-point solution. The converse\nis \u2013 unsurprisingly \u2013 not true. The link between stochastic solutions and the attractor of \u03c6\u2217exhibited in\nProposition 3.3 poses a strong argument in favor of the definition of a stochastic solution through almost\nsure equality of the state equation.\nExample 3.4. Let us demonstrate that stochastic fixed-point solutions that are not stochastic solutions\nexist even in trivial cases. Suppose the state map f(x, u) =xdoes not depend on the input. Let \u03bd\u2208P(X\u2212)\nbe shift-invariant, that is, T\u2217\u03bd=\u03bd, and let \u00b5\u2208P(X\u2212\u00d7 U\u2212)be the product measure of \u03bdand any given\ninput law \u039e\u2208P(U\u2212). Then, \u00b5is a stochastic fixed-point solution but we know from Proposition 3.3 that\n\u00b5\u2208 Sstochif only if \u03bdis supported on the diagonal in X\u2212.\nFor the remainder of the paper, we will use being an element of \u03c4\u2217(S\u03c6\u2217)as a working definition for a\nstochastic solution. Stochastic outputs are elements of Ostoch:=H\u2217(Sstoch).\nDefinition 3.5. We say that the state-space system has the stochastic ESP if there exists a unique\nstochastic solution for any given input law; and we say that it has the stochastic ESP in the outputs if\nthere exists a unique stochastic output for any given input law.\nAs in the deterministic framework, we regard the sets SstochandOstochas their set-valued fiber maps\n\u039e7\u2192 Sstoch\u2229(\u03c0U\u2212)\u22121\n\u2217(\u039e)and\u039e7\u2192 Ostoch\u2229(\u03c0U\u2212)\u22121\n\u2217(\u039e), respectively.\nDefinition 3.6. We say that the state-space system has the stochastic FMP at a given input law \u039e\u2208 P\u2212\nin\nifSstochis hemi-continuous at \u039e. We denote the set of all points at which the state-space system has the\nstochastic FMP by D(Sstoch). We say that the state-space system has the stochastic FMP if D(Sstoch) =P\u2212\nin.\nThestochastic FMP in the outputs and the set D(Ostoch)are defined analogously.\nIn the presence of the ESP and the FMP, Sdetbecomes a continuous map U\u2212\u2192 X\u2212\u00d7 U\u2212and its\npush-forward is well-defined. It was shown in [73, Remark 3.11] that if the ESP, the FMP, and the stochastic\nESP hold, then the unique stochastic solution is given by the push-forward of the input law under Sdet. The\nproof therein presupposed the stochastic ESP, but we would expect that the stochastic ESP is, in fact, a\nconsequence of the ESP and the FMP. Here, we supplement this generalization.\nProposition3.7. Suppose the state-space system has the ESP and the FMP. Then, Sdet\n\u2217(P\u2212\nin)\u2229P\u2212\nstate=Sstoch.\nIn particular, if Sdet\n\u2217(P\u2212\nin)\u2286 P\u2212\nstate, then the state-space system also has the stochastic ESP.\nIn the next result, we generalize Theorem 2.6 and the first part of Theorem 2.15 to the stochastic case.\nTheorem 3.8. Suppose (\u03c0U\u2212)\u2217:P\u2212\nstate\u2192 P\u2212\ninis proper.3If the state-space system has the stochastic ESP\n(in the outputs), then it has the stochastic FMP (in the outputs). Furthermore, if P\u2212\ninandP\u2212\nstateare Polish,\nthen the stochastic FMP holds generically even without the stochastic ESP.\nExample 3.9. Suppose U\u2212andX\u2212are Polish and metrized by dU\u2212anddX\u2212, respectively. The product\ntopology on X\u2212\u00d7 U\u2212is metrized by dX\u2212\u00d7U\u2212=dX\u2212\u25e6\u03c0X\u2212+dU\u2212\u25e6\u03c0U\u2212. Suppose P\u2212\nstateandP\u2212\ninare closed\nsubsets of the Wasserstein- pspaces defined on (X\u2212\u00d7 U\u2212, dX\u2212\u00d7U\u2212)and(U\u2212, dU\u2212), respectively, for some\np\u2208[1,\u221e). IfX\u2212is compact, then (\u03c0U\u2212)\u2217:P\u2212\nstate\u2192 P\u2212\ninis proper. We emphasize that compactness of X\u2212\nis sufficient but not necessary.\n3.2 Causal solutions\nAssumption 3.10. For the remainder of this section, suppose that the topologies on UandU\u2212are the\nproduct topologies and that U,U\u2212, andX\u2212are Polish.4\n3A map is proper if it is closed and has compact fibers. Equivalent characterizations are derived in Lemma A.1.\n4The product topology on U(U\u2212) is Polish if and only if U(U\u2212) can be written as a countable intersection of open subsets\nofUZ(UZ\u2212) [22].\n10\n\n--- Page 11 ---\nStochastic dynamics learning with state-space systems\nThe stochastic analog of Proposition 2.9 (generic fading memory) is not as straightforward as the deter-\nministic result. It requires us to consider only solutions with a special probabilistic dependence structure.\nIn a deterministic setting, it is clear that past states are not influenced by future inputs. This is no longer\nthe case in the stochastic setting. Past inputs, which determine past states, are in general correlated to\nfuture inputs, thus creating correlation between past states and future inputs. However, it is natural to\nassume that there is no \u2018direct\u2019 correlation between past states and future inputs but only such correlation\nthat arises through past inputs. We will call this causality. The notion of conditional independence of\nsigma-algebras captures this idea mathematically. The reader unfamiliar with this notion may momentarily\njump to Section 5.3 further below, where the definition is stated, and consult [43, Chapter 8] and [18, Chapter\n7.3] for detailed introductions.\nDefinition 3.11. For any t\u2208Z\u2212, let\u03a3X,tand\u03a3U,tbe the sigma-algebras on X\u2212\u00d7 U\u2212generated by the\nmaps T\u2212t\u25e6\u03c0X\u2212andT\u2212t\u25e6\u03c0U\u2212, respectively. We call a measure \u00b5\u2208P(X\u2212\u00d7 U\u2212)causalif\u03a3X,tand\u03a3U,0\nare conditionally independent given \u03a3U,twith respect to \u00b5for all t\u2208Z\u2212. The set of all causal measures is\ndenoted Pcausal(X\u2212\u00d7 U\u2212).\nExample 3.12. (i) Suppose V:U\u2212\u2192 X\u2212is measurable and time-invariant, that is, V\u25e6T=T\u25e6V. Then,\n(V\u00d7idU\u2212)\u2217\u039eis causal for any \u039e\u2208P(U\u2212). (ii) Suppose Zis another completely regular Hausdorff space,\nV:ZZ\u2212\u2192 U\u2212is measurable and time-invariant, Zis a stochastic process on Zwith mutually independent\nmarginals, and Xis anX\u2212-valued random variable such that (Xs, Zs)s\u2264tis independent of (Zt+1, . . . , Z0)\nfor any t\u2264 \u22121. Then, the joint law of (X, V(Z))is causal. Causal measures of this type appeared in the\nrelated work [73].\nLet us denote the sets of causal stochastic solutions and outputs by Sc,stoch=Sstoch\u2229Pcausal(X\u2212\u00d7 U\u2212)\nandOc,stoch=H\u2217(Sc,stoch). Thecausal stochastic ESP and thecausal stochastic FMP are defined in\nthe obvious analogous ways. The following result is known in the deterministic case [64, Proposition 3]. We\nprove that it extends to the stochastic case.\nProposition 3.13. Suppose U=\u03c4\u22121(U\u2212)and that the state-space system has the causal stochastic ESP.\nThen, the causal solution to a shift-periodic input is also shift-periodic with the same minimal period.\nUnder a compactness assumption, causal solutions always exist for any given input law. In particular,\nunique solutions are automatically causal.\nProposition 3.14. Suppose (\u03c0U\u2212)\u2217:P\u2212\nstate\u2192 P\u2212\ninhas compact fibers. Then, Sc,stochhas non-empty fibers.\nIn particular, if the state-space system has the stochastic ESP (in the outputs), then it also has the causal\nstochastic ESP (in the outputs).\nExample 3.15. Let us demonstrate that not all stochastic solutions are causal. Suppose the state map\nf(x, u) =xdoes not depend on the input. Let (X, U)be an X\u2212\u00d7 U\u2212-valued random variable such that U0\nis not T(U)-measurable and Xt=U0almost surely for all t\u2208Z\u2212. Then, their joint law belongs to Sstoch\nand is not causal.\nIt is an open problem whether the causal analog of Theorem 3.8 holds, that is, whether the causal\nstochastic ESP implies the causal stochastic FMP. However, using causality, we are able to recover the\ngeneric fading memory in the stochastic case. For the remaining two results in this subsection, we add the\nfollowing assumption.\nAssumption 3.16. For any \u039e,\u039e\u2032\u2208 P\u2212\nin, suppose that \u03b3n\n\u2217(\u039e\u2032\u2297\u039e)converges to \u039easn\u2192 \u221ein the topology\nofP\u2212\nin, where \u03b3n:U\u2212\u00d7 U\u2212\u2192 U\u2212denotes the map \u03b3n(u\u2032, u) = (. . . , u\u2032\n\u22121, u\u2032\n0, u\u2212n, . . . , u0).\nRemark 3.17. The map \u03b3nis continuous by Assumption 2.1.(iii). Furthermore, we know from Assump-\ntion 2.1.(iv) that \u03b3n(u\u2032, u)converges point-wise to uasn\u2192 \u221e. Thus, if the topology on P\u2212\ninis the weak\ntopology, then Assumption 3.16 is satisfied.\nProposition 3.18. The map D(Sc,stoch)\u2192N0\u222a {\u221e},\u039e7\u2192#Sc,stoch(\u039e)is constant. Furthermore, if we\ndenote this constant by M, then #Sc,stoch(\u039e)\u2265Mfor any \u039e\u2208 P\u2212\nin. In particular, if the state-space system\nhas the causal stochastic FMP, then #Sc,stoch(\u039e)is constant on P\u2212\nin, and if it has the causal stochastic FMP\nand#Sc,stoch(\u039e) = 1for some \u039e\u2208 P\u2212\nin, then it has the causal stochastic ESP.\n11\n\n--- Page 12 ---\nOrtega and Rossmannek\nAs in the deterministic case, to extend this result to the outputs, we pose a control-theoretic condition.\nLetC:={(h(x\u22121), u):(x, u)\u2208 Sdet}. Note that the state-space system distinguishes reachable states if\nand only if there exists a function g:C\u2192 Ythat satisfies g(h(x\u22121), u) =h(x0)for all (x, u)\u2208 Sdet. This\nmotivates the next definition.\nDefinition 3.19. We say that the state-space system measurably distinguishes reachable states if the\nsetCis Borel measurable and there exists a measurable function g:C\u2192 Ythat satisfies g(h(x\u22121), u) =h(x0)\nfor all (x, u)\u2208 Sdet.\nProposition 3.20. If the state-space system measurably distinguishes reachable states, then the analog\nstatement of Proposition 3.18 holds with Oc,stochin place of Sc,stoch.\nIn Theorem 2.15, we uncovered that bifurcations of deterministic solutions can only occur within a meager\nset. This was obtained by combining the generic FMP with the result that the number of solutions is constant\non the domain of the FMP. In the stochastic context, we saw in Theorem 3.8 that the stochastic FMP holds\ngenerically. However, the analog claim about bifurcations of solutions is proved only for causal stochastic\nsolutions. With causality, we do not know if the causal stochastic FMP holds generically. Thus, we cannot\ninfer the same result as Theorem 2.15 about bifurcations of stochastic solutions on a meager set. We pose\nthis open problem as a conjecture.\nConjecture 3.21. Suppose that P\u2212\ninandP\u2212\nstateare Polish and that (\u03c0U\u2212)\u2217:P\u2212\nstate\u2192 P\u2212\ninis proper. Then,\n#Sc,stoch(\u039e)is generically constant, and so is #Oc,stoch(\u039e)if the state-space system measurably distinguishes\nreachable states.\n4. On abstract dynamical systems\nThe results on state-space systems that we presented in the previous sections can (in part) be proved in a\nmore general framework for abstract dynamical systems. On the one hand, this gives us insights on exactly\nwhich properties of state-space systems are the ones that enable us to prove said results. On the other hand,\nwe will see that both the deterministic and the stochastic results can be accommodated in one unifying\ntheory.\nIn Section 2.1, we teased a connection between solutions and attractors on the level of abstract dynamical\nsystems. This is the content of Section 4.1. Dynamical systems are broadly categorized in autonomous\nand non-autonomous ones. We review how the notions of a solution for these systems are related to each\nother and propose a unifying framework. Being intimately related to these solutions, we will also encounter\ngeneralized synchronizations [23,49]. The global attractor of a dynamical system will make its appearance\nnaturally along the way.\nIn Section 4.2, we prove the majority of results about how sets of solutions depend on a base point,\nwhich corresponds to the input in the context of state-space systems, ranging from continuous dependence to\nbifurcations. Thereafter, Section 4.3 discusses the distributional behavior of a random state of the dynamical\nsystem.\n4.1 Solutions of dynamical systems\nThenotionsofasolutionofadynamicalsysteminthefollowingthreesubsectionsarestandardmaterial[47,48]\nbut need to be reviewed to set the stage for our proposed unifying framework in Section 4.1.4.\n4.1.1 Autonomous systems\nLet\u03a6:X\u2192Xbe an autonomous dynamical system. A subset A\u2286Xis called forward-invariant if \u03a6(A)\u2286A\nand strictly invariant if \u03a6(A) =A. Recall that a solution of\u03a6through x\u2208Xis a bi-infinite trajectory\nthrough x, that is, a map \u03c7:Z\u2192Xsatisfying \u03c7(0) = xand\u03c7(t+n) = \u03a6n(\u03c7(t))for all t\u2208Zandn\u2208N\n(but it suffices to check this for n= 1). We denote the set of solution points as\nS\u03a6:={x\u2208X:there exists a solution of \u03a6through x}.\nIt is not difficult to show that S\u03a6is the maximal strictly invariant set, that is, S\u03a6is strictly invariant and\nany other strictly invariant subset A\u2286Xis contained in S\u03a6. In particular, S\u03a6contains all equilibria and\nperiodic orbits. If \u03a6is injective and \u0398:X\u2192Xis a left-inverse of \u03a6, then \u03c7:Z\u2192Xis a solution of \u03a6\n12\n\n--- Page 13 ---\nStochastic dynamics learning with state-space systems\nthrough xif and only if \u03c7(n) = \u03a6n(x),\u03c7(\u2212n) = \u0398n(x), and \u03a6n(\u0398n(x)) =xfor all n\u2208N0. In this case, S\u03a6\nbecomes the maximal set on which \u0398nis also a right-inverse of \u03a6nfor any n\u2208N;\nS\u03a6={x\u2208X: \u03a6n(\u0398n(x)) =xfor all n\u2208N}.\nIn particular, if \u03a6is injective, then it follows easily that S\u03a6=T\nn\u2208N\u03a6n(X). There are competing definitions\nof an attractor of a dynamical system [68,70], discussing which goes beyond the scope of this paper. In the\npresent paper, we callT\nn\u2208N\u03a6n(X)theglobal attractor . Thus, if \u03a6is injective, then S\u03a6coincides with the\nglobal attractor. We emphasize that injectivity is sufficient but not necessary for this.\n4.1.2 Non-autonomous systems: processes\nDenote Z2\n\u2265:={(t1, t0)\u2208Z\u00d7Z:t1\u2265t0}. Let \u03a8:Z2\n\u2265\u00d7 X \u2192 X be a non-autonomous dynamical system in\nthe language of processes, that is, a map that satisfies the initial value condition \u03a8(t0, t0, x) =xand the\nevolution property \u03a8(t2, t0, x) = \u03a8( t2, t1,\u03a8(t1, t0, x))for all t2\u2265t1\u2265t0. Since the evolution of a process\ndepends on the initial time, it matters at which time a solution passes through a point. A solution of \u03a8\nthrough x\u2208 Xat time t\u2208Zis a map \u03c7:Z\u2192 Xthat satisfies \u03c7(t) =xand\u03c7(t1) = \u03a8( t1, t0, \u03c7(t0))for all\n(t1, t0)\u2208Z2\n\u2265. The set of solution points becomes a subset of X \u00d7Z, namely\nSprocess\n\u03a8 :={(x, t)\u2208 X \u00d7 Z:there exists a solution of \u03a8through xat time t}.\nIt is well-known that a process can be turned into an autonomous dynamical system by augmenting the state\nspace with a time coordinate. Specifically, the map \u03a6:X \u00d7Z\u2192 X \u00d7 Z,(x, t)7\u2192(\u03a8(t+ 1, t, x), t+ 1)satisfies\n\u03a6t1\u2212t0(x, t0) = (\u03a8( t1, t0, x), t1)for all (t1, t0)\u2208Z2\n\u2265. If\u03c7is a solution of the process \u03a8through xat time t,\nthen the augmented map \u03c7\u2032:Z\u2192 X \u00d7 Z,s7\u2192(\u03c7(t+s), t+s)is a solution of the autonomous dynamical\nsystem \u03a6through (x, t). Conversely, if \u03c7\u2032is a solution of \u03a6through (x, t), then the projection of \u03c7\u2032to the\nX-coordinate is a solution of \u03a8through xat time t. Thus, Sprocess\n\u03a8 =S\u03a6.\n4.1.3 Non-autonomous systems: skew-products\nLet\u03d5:Z \u2192 Z be an invertible autonomous dynamical system, and let \u03a8:N0\u00d7 Z \u00d7 X \u2192 X be a non-\nautonomous dynamical system driven by \u03d5in the language of skew-products, that is, a map that satisfies\nthe initial value condition \u03a8(0, z, x) =xand the co-cycle property \u03a8(n+m, z, x ) = \u03a8( n, \u03d5m(z),\u03a8(m, z, x ))\nfor all m, n\u2208N0. Let orb\u03d5(z) :=S\nt\u2208Z{\u03d5t(z)}denote the bi-infinite orbit of a point z\u2208 Zunder \u03d5. In the\nskew-product formalism, a solution of \u03a8through x\u2208 Xwith input z\u2208 Zis a map \u03c7: orb \u03d5(z)\u2192 Xthat\nsatisfies \u03c7(z) =xand\u03c7(\u03d5t1(z)) = \u03a8( t1\u2212t0, \u03d5t0(z), \u03c7(\u03d5t0(z)))for all (t1, t0)\u2208Z2\n\u2265. The set of solution points\nis analogously\nSskew -product\n\u03a8 :={(x, z)\u2208 X \u00d7 Z :there exists a solution of \u03a8through xwith input z}.\nAsforprocesses, wemayaugmentthestatespaceoftheskew-productandconsidertheautonomousdynamical\nsystem \u03a6:X \u00d7 Z \u2192 X \u00d7 Z ,(x, z)7\u2192(\u03a8(1, z, x), \u03d5(z)), which satisfies \u03a6n(x, z) = (\u03a8( n, z, x ), \u03d5n(z))for all\nn\u2208N0. If \u03c7is a solution of the skew-product \u03a8through xwith input z, then the augmented map\n\u03c7\u2032:Z\u2192 X \u00d7 Z ,t7\u2192(\u03c7(\u03d5t(z)), \u03d5t(z))is a solution of \u03a6through (x, z). Unlike for processes, the converse\ndoes not hold in general. Thus, Sskew -product\n\u03a8 \u2286 S\u03a6, and the inclusion may be strict. In the next section, we\ncharacterize the points that belong to S\u03a6but not to Sskew -product\n\u03a8 .\nMore common in the literature is the notion of an entire solution \u03c7:Z \u2192 X, which is defined by the\nsame characteristic property \u03c7(\u03d5t1(z)) = \u03a8( t1\u2212t0, \u03d5t0(z), \u03c7(\u03d5t0(z)))but has domain Z. Clearly, if \u03c7is an\nentire solution, then its restriction to orb\u03d5(z)is a solution of \u03a8through \u03c7(z)with input z. We could denote\nSentire\n\u03a8 :={(x, z)\u2208 X \u00d7 Z :there exists an entire solution \u03c7of\u03a8with \u03c7(z) =x} \u2286 Sskew -product\n\u03a8 .\nThe subtlety of entire solutions lies in the dependence of existence of solutions for various inputs: if a solution\nwith input zdoes not exist for some z\u2208 Z, then neither does an entire solution exist, and then Sentire\n\u03a8is\nempty even if Sskew -product\n\u03a8 is non-empty. This makes the concept of an entire solution useful only if we have\nsome additional knowledge of the dynamical system that guarantees the existence of at least one solution\nwith each input. If this guarantee is met, then Sentire\n\u03a8 =Sskew -product\n\u03a8 .\nThe non-autonomous dynamical system \u03a8has theunique solution property (USP) if for each z\u2208 Z\nthere exists a unique solution of \u03a8with input z. The USP is equivalent to the existence of a uniqueentire\nsolution. In this case, that unique entire solution is called the generalized synchronization [49].\n13\n\n--- Page 14 ---\nOrtega and Rossmannek\n4.1.4 Autonomous systems on bundles\nWe introduce an overarching framework. Consider a map \u03c0:X\u2192 Zand an invertible autonomous dynamical\nsystem \u03d5:Z \u2192 Z. Abundle map over\u03d5is a map \u03a6:X\u2192Xthat satisfies \u03c0\u25e6\u03a6 =\u03d5\u25e6\u03c0. The autonomous\ndynamical system obtained from a skew-product in the previous section fits the special case X=X \u00d7 Z.\nThe autonomous dynamical system obtained from a process can be regarded as a bundle map over Z\u2192Z,\nt7\u2192t+ 1. In fact, any autonomous dynamical system can be regarded as a bundle map by taking Zto be a\none-point space if no additional structure is available.\nExample 4.1. The push-forward \u03c6\u2217:Pstate\u2192 P statefrom Section 3 is a bundle map over \u03c3\u2217:Pin\u2192 P in\non the bundle (\u03c0U)\u2217:Pstate\u2192 P in. This dynamical system cannot be written as a skew-product since\nP(X\u2212\u00d7U)\u0338=P(X\u2212)\u00d7P(U). Thisexemplifiestheneedforautonomoussystemsonbundlesasageneralization\nof skew-products.\nRemark 4.2. Suppose \u03a6is injective. Then, S\u03a6=T\nn\u2208N\u03a6n(X)as noted in Section 4.1.1. Since \u03a6is a\nbundle map over \u03d5, we have S\u03a6\u2229\u03c0\u22121(z) =T\nn\u2208N\u03a6n(\u03c0\u22121(\u03d5\u2212n(z))). Thus, if \u03a6encodes a non-autonomous\ndynamical system, say by augmentation, then S\u03a6corresponds to its global pullback attractor.\nSince \u03a6is an autonomous dynamical system, the initial notion of solution from Section 4.1.1 persists. A\nsynchronized solution of\u03a6through x\u2208Xis a map \u03c7: orb \u03d5(\u03c0(x))\u2192Xthat satisfies \u03c7(\u03c0(x)) = xand\n\u03a6\u25e6\u03c7=\u03c7\u25e6\u03d5. The denomination \u2018synchronized\u2019 is motivated by the fact that \u03c7is a semi-conjugacy between\n\u03a6and\u03d5on the orbit of z. Let\nSsync\n\u03a6={x\u2208X:there exists a synchronized solution of \u03a6through x}.\nIf\u03c7is a synchronized solution of \u03a6through x, then \u03c7\u2032:Z\u2192X,t7\u2192\u03c7\u25e6\u03d5t\u25e6\u03c0(x)is a solution of \u03a6\nthrough x. Thus, Ssync\n\u03a6\u2286 S \u03a6. If\u03a6is the autonomous dynamical system obtained from augmenting a\nskew-product \u03a8, then Ssync\n\u03a6=Sskew -product\n\u03a8 . To characterize S\u03a6\\Ssync\n\u03a6, we need to consider periodic points.\nLetPer(\u03a6)andPer(\u03d5)denote the set of periodic points of \u03a6and\u03d5. Since \u03a6is a bundle map over \u03d5,\nwe have Per(\u03a6)\u2286\u03c0\u22121(Per(\u03d5)), and the inclusion may be strict. Similarly, let Aper (\u03a6) = X\\Per(\u03a6)and\nAper (\u03d5) =Z\\Per(\u03d5)be the set of aperiodic points of \u03a6and\u03d5. Introduce the set of synchronized periodic\npointsof\u03a6and\u03d5as the set of all periodic points xof\u03a6whose minimal period is not greater than the\nminimal period of \u03c0(x)under \u03d5, that is,\nSync-Per(\u03a6 , \u03d5) :={x\u2208X:\u2203n\u2208N: \u03a6n(x) =xand\u22001\u2264m < n :\u03d5m(\u03c0(x))\u0338=\u03c0(x)}.\nHeuristically, if we drive a system with a periodic input, we would hope that the system creates a stable\nresponse to the input with the same periodicity. The next lemma shows that this heuristic is captured by\nthe synchronized solutions.\nProposition 4.3. The set of synchronized solution points is characterized by\nSsync\n\u03a6=\u0010\nS\u03a6\u2229\u03c0\u22121(Aper( \u03d5))\u0011\n\u222aSync-Per(\u03a6 , \u03d5)\nand, hence,\nS\u03a6\\Ssync\n\u03a6=\u0010\nS\u03a6\u2229Aper(\u03a6) \u2229\u03c0\u22121(Per( \u03d5))\u0011\n\u222a\u0010\nPer(\u03a6) \\Sync-Per(\u03a6 , \u03d5)\u0011\n.\nProof.Observe that S\u03a6is the disjoint union of the four sets S0:=S\u03a6\u2229\u03c0\u22121(Aper (\u03d5)),S1:=Sync-Per(\u03a6, \u03d5),\nS2:=S\u03a6\u2229Aper (\u03a6)\u2229\u03c0\u22121(Per(\u03d5)), and S3:=Per(\u03a6)\\Sync-Per(\u03a6, \u03d5). First, if \u03c7is a synchronized solution\nof\u03a6through xand if z:=\u03c0(x)is periodic under \u03d5with period n\u2208N, then \u03a6n(x) = \u03a6n\u25e6\u03c7(z) =\u03c7\u25e6\u03d5n(z) =\n\u03c7(z) =x. This shows that S2\u222aS3\u2286 S\u03a6\\Ssync\n\u03a6. Secondly, if x\u2208Sync-Per(\u03a6, \u03d5)and if n\u2208Nis the minimal\nperiod of z:=\u03c0(x)under \u03d5, then the map {z, \u03d5(z), . . . , \u03d5n\u22121(z)} \u2192X,\u03d5t(z)7\u2192\u03a6t(x)defines a synchronized\nsolution. Thus, S1\u2286 Ssync\n\u03a6. Lastly, if \u03c7:Z\u2192Xis a solution of \u03a6through xand if \u03c0(x)\u2208Aper (\u03d5), then the\nmap orb\u03d5(\u03c0(x))\u2192X,\u03d5t(\u03c0(x))7\u2192\u03c7(t)is well-defined and a synchronized solution. Thus, S0\u2286 Ssync\n\u03a6.\nOn the one hand, if \u03a6is a bundle map over \u03d5:Z\u2192Z,t7\u2192t+1, e.g. obtained from augmenting a process,\nthen Per(\u03d5)is empty and S\u03a6=Ssync\n\u03a6. On the other hand, it is easy to construct examples of skew-products\nsuch that the bundle map obtained from augmentation admits points in either S\u03a6\u2229Aper (\u03a6)\u2229\u03c0\u22121(Per(\u03d5))\norPer(\u03a6)\\Sync-Per(\u03a6, \u03d5). In particular, the maximal invariant set of a skew-product is, in general, not equal\nto the collection of solutions of the system. This corrects a remark made in [46].\n14\n\n--- Page 15 ---\nStochastic dynamics learning with state-space systems\nExtending the notion for skew-products, we can introduce an entire solution as a map \u03c7:Z \u2192Xthat\nsatisfies \u03a6\u25e6\u03c7=\u03c7\u25e6\u03d5and\u03c0\u25e6\u03c7=idZ; and we can say that \u03a6has theunique solution property (USP) if\nfor every z\u2208 Zthere exists a unique x\u2208\u03c0\u22121(z)through which there exists a solution of \u03a6. In other words,\n\u03a6has the USP if the fiber S\u03a6\u2229\u03c0\u22121(z)is a singleton for all z\u2208 Z. The same comment applies that we made\nfor entire solutions of skew-products: if there exists some z\u2208 Zsuch that through any x\u2208\u03c0\u22121(z)no solution\nexists, then neither does an entire solution exist. However, the USP is equivalent to the existence of a unique\nentire solution, which is then the unique left-inverse of \u03c0with range S\u03a6. As before, in this case, that unique\nentire solution is called the generalized synchronization (GS) . In the language of autonomous systems\non bundles, by definition, the generalized synchronization is a semi-conjugacy between the base dynamical\nsystem \u03d5and the restriction of the dynamical system \u03a6toS\u03a6. In particular, that the dynamics of \u03a6onS\u03a6\nbecome synchronized with \u03d5implies that S\u03a6\\Ssync\n\u03a6is empty, which we make rigorous in the corollary below.\nThis corollary generalizes [64, Proposition 3].\nCorollary 4.4. Suppose \u03a6has the USP. Then, S\u03a6=Ssync\n\u03a6.\nProof.By Proposition 4.3, we have to show that S\u03a6\u2229\u03c0\u22121(Per(\u03d5))\u2286Sync-Per(\u03a6, \u03d5). Let \u03c7:Z \u2192Xbe\nthe generalized synchronization, and let x\u2208 S\u03a6. Ifz:=\u03c0(x)is periodic under \u03d5with period n\u2208N, then\n\u03a6n(x) = \u03a6n\u25e6\u03c7(z) =\u03c7\u25e6\u03d5n(z) =\u03c7(z) =x, which shows that x\u2208Sync-Per(\u03a6 , \u03d5).\nBeing a right-inverse of \u03c0, the generalized synchronization \u2013 if it exists \u2013 is always injective. If X=X \u00d7Z\nis a product space, then the GS is a map of the form \u03c7=\u03b6\u00d7idZwith \u03b6:Z \u2192 X. The GS for the\nskew-product is the standalone map \u03b6:Z \u2192 X. The map \u03b6is not necessarily injective, but if it is, then\nits left-inverse is a conjugacy between the true underlying dynamics of \u03d5and the dynamics of the states of\nthe skew-product on its pullback attractor. The GS of an autonomous system on a bundle also defines a\nconjugacy but not the desired one, because the \u2018extended states\u2019 in X \u00d7 Zcontain information about the\ninputs.\nTo unify the concept of the GS, we have to compose the GS of\nan autonomous system on a bundle with the projection map X \u00d7\nZ \u2192 X. Generalizing this idea to the case of non-product spaces\nX, we have to compose the GS \u03c7:Z \u2192Xwith a map \u03a0:X\u2192 X,\nwhich we think of as a state-extraction map that forgets the\nencoding of the input. Now, if \u03a0\u25e6\u03c7happens to be injective with\nleft-inverse \u03b7:\u03a0(S\u03a6)\u2192 Z, then we get the commutative diagram\non the right, where \u02c6\u03a6= \u03a0\u25e6\u03a6\u25e6\u03c7\u25e6\u03b7:\u03a0(S\u03a6)\u2192\u03a0(S\u03a6)captures\nthe \u2018states-only\u2019 dynamics of \u03a6. In particular, \u03b7is a conjugacy\nbetween \u03d5and\u02c6\u03a6.\u03a0(S\u03a6) \u03a0(S\u03a6)\nS\u03a6 S\u03a6\nZ Z\u03b7\u02c6\u03a6\n\u03b7\u03a0\n\u03c0\u03a6\n\u03c0\u03a0\n\u03c7\n\u03d5\u03c7\n4.1.5 Parametrized autonomous systems\nConsider a parametrized family (\u03a6z)z\u2208Zof autonomous dynamical systems \u03a6z:X \u2192 X. Let X=X \u00d7 Z\nand\u03c0=\u03c0Z:X\u2192 Zbe the projection. Then, \u03a6(x, z) := (\u03a6 z(x), z)is a bundle map X\u2192Xover the identity\nZ \u2192 Z. It is easy to see that S\u03a6z\u00d7 {z}=S\u03a6\u2229\u03c0\u22121\nZ(z). Thus, a parametrized family of solution sets can be\ntranslated to the solution set of a single bundle map parametrized by its fibers.\nWe show that the converse can (almost) be done even if \u03a6:X \u00d7Z \u2192 X \u00d7Z is a bundle map over a bijection\n\u03d5:Z \u2192 Zthat is not necessarily the identity. For all z\u2208 Z, consider the autonomous dynamical system\n\u03a6z:X \u00d7Z\u2192 X \u00d7 Zgiven by \u03a6z(x, t) = (\u03c0X\u25e6\u03a6(x, \u03d5t(x)), t+ 1)(which is the autonomous system obtained\nfrom augmenting the process \u03a8zonXdetermined by \u03a8z(t+ 1, t, x) =\u03c0X\u25e6\u03a6(x, \u03d5t(x))). If\u03c7:Z\u2192 X \u00d7 Z\nis a solution of \u03a6zthrough (x,0), then \u03c7\u2032:Z\u2192X,t7\u2192(\u03c0X\u25e6\u03c7(t), \u03d5t(z))is a solution of \u03a6through (x, z).\nConversely, if \u03c7:Z\u2192 X \u00d7 Z is a solution of \u03a6through (x, z), then \u03c7\u2032:Z\u2192 X \u00d7 Z,t7\u2192(\u03c0X\u25e6\u03c7(t), t)is\na solution of \u03a6zthrough (x,0). Thus, the fiber S\u03a6\u2229\u03c0\u22121\nZ(z)is in bijection with the fiber S\u03a6z\u2229\u03c0\u22121\nZ(0)via\n(x, z)7\u2192(x,0). If\u03d5is not the identity, we cannot get around making the domain of \u03a6zpartially discrete\nsince \u03a6encodes non-autonomous dynamics.\nThe preceding paragraphs show that parametrized families of autonomous dynamical systems and au-\ntonomous systems on trivial bundles are equivalent if we are interested in their solution sets. However,\nsystems on bundles become more general when Xis not a product space X \u00d7 Zas in Example 4.1.\n15\n\n--- Page 16 ---\nOrtega and Rossmannek\n4.2 Dependence of solutions on base points\nFor the remainder of Section 4, let XandZbe Hausdorff spaces and \u03c0:X\u2192 Zbe a continuous map. Let\n\u03d5:Z \u2192 Zbe a continuous bijection and \u03a6be a continuous bundle map over \u03d5, that is, \u03c0\u25e6\u03a6 =\u03d5\u25e6\u03c0. In\nthis section, we study how the fibers of S\u03a6vary with the base point.\n4.2.1 Continuity of set of solutions\nAn important question in dynamical systems theory is whether the fibers of the global attractor depend\ncontinuously on the base point. To address this question rigorously, we recall the notion of hemi-continuity,\nwhich generalizes the concept of continuity to set-valued maps in general topological spaces [6]. Given a\npoint z0\u2208 Z, a set-valued map S:Z \u2192 2Xisupper hemi-continuous atz0if for any open set W\u2286X\ncontaining S(z0)there exists an open set V\u2286 Zcontaining z0such that S(z)\u2286Wfor all z\u2208V;lower\nhemi-continuous atz0if for any open set W\u2286Xwith W\u2229S(z0)\u0338=\u2205there exists an open set V\u2286 Z\ncontaining z0such that W\u2229S(z)\u0338=\u2205for all z\u2208V; andhemi-continuous atz0if it is both upper and\nlower hemi-continuous at z0. A set-valued map is (upper/lower) hemi-continuous if it is (upper/lower) hemi-\ncontinuous at all points.5Note that if S(z)is a singleton for any z\u2208 Z, then upper hemi-continuity, lower\nhemi-continuity, hemi-continuity of S, and usual continuity of Sas a map Z \u2192Xare all equivalent. Using the\nbundle structure, by a slight abuse of notation, we say that a subset S\u2286Xis (upper/lower) hemi-continuous\nif the set-valued map z7\u2192S\u2229\u03c0\u22121(z)is (upper/lower) hemi-continuous. Under some assumptions on X,Z,\nand\u03c0, closed subsets turn out to be automatically upper hemi-continuous and generically hemi-continuous;\nsee Proposition 4.5 below. Recall that a map is proper if it is closed and has compact fibers; and that a\nsubset of a topological space is called residual if it can be written as the intersection of countably many\nsubsets each of which has a dense interior.\nProposition 4.5. Suppose \u03c0is proper, and let S\u2286Xbe closed. Then, Sis upper hemi-continuous.\nFurthermore, if XandZare Polish, then the set of points in Zat which Sis hemi-continuous is residual.\nProof.Suppose for contradiction that Sis not upper hemi-continuous. This entails that there exist an\nelement z\u2208 Z, an open set W\u2286Xcontaining S\u2229\u03c0\u22121(z), a net (zi)i\u2208Iindexed by the open neighborhoods\nofzand converging to z, and for each i\u2208Ian element xi\u2208S\u2229\u03c0\u22121(zi)\u2229Wc. Since \u03c0is proper, (xi)i\u2208I\nadmits some cluster point x\u2208\u03c0\u22121(z); see Lemma A.1. Being a closed subset, S\u2229Wcmust contain x, which\ncontradicts x\u2208S\u2229\u03c0\u22121(z)\u2286W. This shows that Sis upper hemi-continuous. It is known that an upper\nhemi-continuous map from a completely metrizable space to subsets of a Polish space is hemi-continuous on\na residual set [6, Theorem 1.4.13].\nGiven a parametrized family of autonomous dynamical systems, it has been studied when the individual\nattractors depend hemi-continuously on the parameter [7,21,37,47]. With our discussion in Section 4.1.5,\nwe are able to recover those results that prove residual continuity of attractors of continuously parametrized\ndynamical systems on a compact Polish space.\nProposition 4.6. Suppose Zis Polish and \u03a6z:X \u2192 X,z\u2208 Z, are autonomous dynamical systems on a\ncompact Polish space Xsuch that X \u00d7 Z \u2192 X ,(x, z)7\u2192\u03a6z(x)is continuous. Then, S\u03a6z=T\nn\u2208N\u03a6n\nz(X)for\nallz\u2208 Z, and the set S\u03a6zdepends hemi-continuously on zin a residual set.\nProof.As in Section 4.1.5, consider X=X \u00d7Zand\u03a6:X \u00d7Z \u2192 X \u00d7Z ,(x, z)7\u2192(\u03a6z(x), z). Note that the\nprojection X \u00d7 Z \u2192 Z is proper and the sets \u03a6n(X \u00d7 Z ),n\u2208N, are closed by compactness of X. We show\nthatS\u03a6=T\nn\u2208N\u03a6n(X \u00d7Z ) =:A. It is easy to see that S\u03a6is contained in A. Since S\u03a6is the maximal strictly\ninvariant set, it therefore suffices to show that Ais strictly invariant. That Ais forward-invariant is clear.\nThatA \u2286 \u03a6(A)can be shown by a standard argument, which we briefly repeat here. Suppose (x, z)\u2208 A.\nIn particular, there exists a sequence (xn)n\u2208N\u2286 Xwith (xn, z)\u2208\u03a6n(X \u00d7 Z )and(x, z) = \u03a6( xn, z). By\ncompactness, (xn)n\u2208Nhas some convergent subnet with limit (x\u2032, z). Since \u03a6n(X \u00d7 Z )is a nested sequence\nof closed sets, we necessarily have (x\u2032, z)\u2208 A. By continuity, (x, z) = \u03a6( x\u2032, z)\u2208\u03a6(A). We have shown that\nS\u03a6=Ais the intersection of closed sets, hence itself closed. This enables us to apply Proposition 4.5 to\nconclude that S\u03a6zdepends hemi-continuously on zin a residual set. The same argument as above shows\nthatS\u03a6z=T\nn\u2208N\u03a6n\nz(X).\n5Here, we consider general set-valued maps on Hausdorff spaces. When Xis a metric space and Sis compact-set-valued,\nthen continuity of Sis typically defined as continuity with respect to the Hausdorff distance, which defines a metric on the\nset of non-empty compact subsets of X. In this case, Sis hemi-continuous if and only if Sis continuous with respect to the\nHausdorff distance.\n16\n\n--- Page 17 ---\nStochastic dynamics learning with state-space systems\nWe conclude this section with an application of Proposition 4.5 to dynamical systems with the USP.\nProposition 4.7. Suppose \u03c0is proper and \u03a6has the USP. Furthermore, suppose Xis compact or \u03a6admits\na continuous left-inverse. Then, S\u03a6=T\nn\u2208N\u03a6n(X)is (hemi-)continuous.\nProof.We pointed out further above that upper hemi-continuity, hemi-continuity, and usual continuity agree\nfor singleton-valued maps. Hence, by Proposition 4.5, we only have to verify that S\u03a6equalsT\nn\u2208N\u03a6n(X)and\nis a closed set. For compact X, this is done exactly as in the proof of Proposition 4.6. We noted in Section 4.1.1\nthat if \u03a6admits a continuous left-inverse, then S\u03a6coincides with the global attractor. Furthermore, the\nexistence of a continuous left-inverse entails that \u03a6is closed; see Lemma A.1. In particular, the global\nattractor is a closed set.\n4.2.2 Bifurcations\nBeyond continuity of the fibers of S\u03a6, another interesting question is whether something can be said about\nthe number of solutions in a fiber as a function of the base point. To this end, we review the effect of lower\nhemi-continuity. Let YandZ\u2212be Hausdorff spaces.\nLemma 4.8. Suppose O:Z\u2212\u21922Yis lower hemi-continuous at z\u2212\u2208 Z\u2212. If(zi\n\u2212)i\u2208I\u2286 Z\u2212is a convergent\nnet with limit z\u2212, then #O(z\u2212)\u2264lim supi#O(zi\n\u2212).\nProof.Suppose for contradiction that #O(z\u2212)>lim supi#O(zi\n\u2212) =: M. In particular, M <\u221e. Take\nM+ 1distinct points y0, . . . , y M\u2208 O(z\u2212). Since Yis Hausdorff, we can take disjoint open neighborhoods\nWm\u2286Yofym. By lower hemi-continuity, there exists an i0\u2208Isuch that Wm\u2229 O(zi\n\u2212)\u0338=\u2205for all i\u2265i0\nand all 0\u2264m\u2264M. In other words, O(zi\n\u2212)contains at least one point in each of the M+ 1disjoint sets\nW0, . . . , W M, which contradicts lim supi#O(zi\n\u2212) =M.\nThe hypotheses in the next result may seem artificial but they fit precisely the framework of state-space\nsystems as we will see later.\nProposition 4.9. Suppose that \u03a6admits a continuous left-inverse \u0398:X\u2192Xand that there exist maps\n\u03c4:Z \u2192 Z \u2212and\u03b7:X\u2192Yand a subset X\u221e\u2286Xthat is forward-invariant under \u0398such that the following\nhold.\n(i)For any x, x\u2032\u2208 S\u03a6\u2229X\u221e, if\u03b7(\u0398(x)) =\u03b7(\u0398(x\u2032))and\u03c0(x) =\u03c0(x\u2032), then \u03b7(x) =\u03b7(x\u2032).6\n(ii)For any z, z\u2032\u2208 Zwith\u03c4(z) =\u03c4(z\u2032), it holds that \u03b7(S\u03a6\u2229X\u221e\u2229\u03c0\u22121(z)) =\u03b7(S\u03a6\u2229X\u221e\u2229\u03c0\u22121(z\u2032)).\n(iii)For all z\u2212, z\u2032\n\u2212\u2208 Z\u2212there exist nets (zi)i\u2208I\u2286 Zand(ki)i\u2208I\u2286N0such that \u03c4(zi)\u2192z\u2212and\n\u03c4(\u03d5\u2212ki(zi)) =z\u2032\n\u2212for all i\u2208I.\nLetZ\u2217\n\u2212\u2286 Z\u2212be the set of points at which z\u22127\u2192 O (z\u2212) := \u03b7(S\u03a6\u2229X\u221e\u2229(\u03c4\u25e6\u03c0)\u22121(z\u2212))is lower hemi-\ncontinuous. Then, #O(z\u2212)\u2264#O(z\u2032\n\u2212)for any z\u2212\u2208 Z\u2217\n\u2212and any z\u2032\n\u2212\u2208 Z\u2212. In particular, #O(z\u2212)is\nconstant on Z\u2217\n\u2212.\nProof.Fixz\u2212\u2208 Z\u2217\n\u2212andz\u2032\n\u2212\u2208 Z\u2212, andtakenets (zi)i\u2208I\u2286 Zand(ki)i\u2208I\u2286N0asin(iii)suchthat zi\n\u2212:=\u03c4(zi)\nconverges to z\u2212and\u03c4(\u03d5\u2212ki(zi)) =z\u2032\n\u2212for all i\u2208I. By Lemma 4.8, we have #O(z\u2212)\u2264lim supi#O(zi\n\u2212). Fix\ni\u2208I. We claim that #O(zi\n\u2212)\u2264#O(z\u2032\n\u2212). By(ii), there exists a right-inverse \u03b9:O(zi\n\u2212)\u2192 S \u03a6\u2229X\u221e\u2229\u03c0\u22121(zi)\nof\u03b7. The sets S\u03a6andX\u221eare forward-invariant under \u0398. The fiber S\u03a6\u2229\u03c0\u22121(zi)gets mapped into the fiber\nS\u03a6\u2229\u03c0\u22121(\u03d5\u2212ki(zi))under \u0398ki. We show that the composition \u03b7\u25e6\u0398ki\u25e6\u03b9maps the set O(zi\n\u2212)injectively\ninto\u03b7(S\u03a6\u2229X\u221e\u2229\u03c0\u22121(\u03d5\u2212ki(zi))). This is trivial if ki= 0, so we consider the case ki\u22651. To this end,\nsuppose y, y\u2032\u2208 O(zi\n\u2212)satisfy \u03b7\u25e6\u0398ki\u25e6\u03b9(y) =\u03b7\u25e6\u0398ki\u25e6\u03b9(y\u2032). Abbreviate xm= \u0398m\u25e6\u03b9(y)andx\u2032\nm= \u0398m\u25e6\u03b9(y\u2032)\nfor0\u2264m\u2264ki, and note that \u03c0(xm) =\u03d5\u2212m(zi) =\u03c0(x\u2032\nm). From(i), we obtain \u03b7(xki\u22121) =\u03b7(x\u2032\nki\u22121)and\ninductively \u03b7(xki\u22122) =\u03b7(x\u2032\nki\u22122)and so forth until \u03b7(x0) =\u03b7(x\u2032\n0). But \u03b7(x0) =yand\u03b7(x\u2032\n0) =y\u2032, which\nshows that \u03b7\u25e6\u0398ki\u25e6\u03b9is injective on O(zi\n\u2212). Lastly, by (ii), we have \u03b7(S\u03a6\u2229X\u221e\u2229\u03c0\u22121(\u03d5\u2212ki(zi))) =O(z\u2032\n\u2212).\nThis concludes the claim #O(zi\n\u2212)\u2264#O(z\u2032\n\u2212), which yields #O(z\u2212)\u2264#O(z\u2032\n\u2212). This is an equality if z\u2032\n\u2212also\nbelongs to Z\u2217\n\u2212by symmetry of the argument in z\u2212andz\u2032\n\u2212.\nRemark 4.10. It follows easily from the definition of lower hemi-continuity that if \u03c4is open, \u03b7is continuous,\nandZ\u2217\u2286 Zdenotes the set of points at which S\u03a6\u2229X\u221eis lower hemi-continuous, then \u03c4(Z\u2217)\u2286 Z\u2217\n\u2212.\n6Note that this always holds if \u03b7is the identity since x= \u03a6(\u0398( x)) = \u03a6(\u0398( x\u2032)) =x\u2032for all x, x\u2032\u2208 S\u03a6with \u0398(x) = \u0398( x\u2032).\n17\n\n--- Page 18 ---\nOrtega and Rossmannek\n4.2.3 Structured solutions\nIn Proposition 4.9, we introduced a subset X\u221e\u2286Xthat ensured additional properties of the solutions in\nS\u03a6\u2229X\u221e. Next, we study the effect of prescribing a specific structure for the set X\u221e. Throughout this\nsubsection, suppose that \u03a6admits a continuous left-inverse \u0398:X\u2192Xthat is a bundle map over \u03d5\u22121, that\nis,\u03c0\u25e6\u0398 =\u03d5\u22121\u25e6\u03c0, and suppose X0\u2286Xis forward-invariant under \u03a6with closed and non-empty fibers. Let\nX\u221e=T\nn\u2208N0(\u0398n)\u22121(X0). The motivation to introduce such a structure is that the set of causal measures\nthat we encountered in Section 3.2 in the context of stochastic state-space systems can be written in such a\nway. We point out that X\u221emay not be closed since we do not assume X0to be closed.\nLemma 4.11. The fibers of X\u221eare closed. Furthermore, \u03a6\u22121(X\u221e) =X\u221e= \u0398(X\u221e) =X0\u2229\u0398\u22121(X\u221e).\nProof.The assumption \u03c0\u25e6\u0398 =\u03d5\u22121\u25e6\u03c0implies that (\u0398n)\u22121(X0)\u2229\u03c0\u22121(z) = (\u0398n)\u22121(X0\u2229\u03c0\u22121(\u03d5\u2212n(z))). Since\nX0has closed fibers, it follows readily that the same is true of X\u221e. Since \u03a6is a right-inverse of \u0398, we have\n\u03a6\u22121(X\u221e) =X\u221e\u2229\u03a6\u22121(X0). Forward-invariance of X0yields X\u221e\u2286X0\u2286\u03a6\u22121(X0). Hence, \u03a6\u22121(X\u221e) =X\u221e.\nThis also yields X\u221e= \u0398(\u03a6( X\u221e))\u2286\u0398(X\u221e). It is clear that \u0398(X\u221e)\u2286X\u221e=X0\u2229\u0398\u22121(X\u221e).\nSince X\u221emay not be closed, we cannot, in general, apply Proposition 4.5 to S\u03a6\u2229X\u221e.7However, if \u03a6\nhas the USP, then we can show that all solutions belong to X\u221eand, hence, S\u03a6\u2229X\u221e=S\u03a6is continuous.\nProposition 4.12. Suppose the restriction of \u03c0toX0has compact fibers. Then, S\u03a6\u2229X\u221ehas non-empty\nfibers. In particular, if \u03a6has the USP, then S\u03a6\u2286X\u221e.\nProof.Since the fibers of X0are non-empty, there exists a left-inverse \u03c1:X\u2192X0of the inclusion X0,\u2192X\nthat satisfies \u03c0\u25e6\u03c1=\u03c0. Let z\u2208 Zandx\u2208\u03c0\u22121(z). Consider xn= \u03a6n\u25e6\u03c1\u25e6\u0398n(x),n\u2208N0, which satisfy\n\u03c0(xn) =zsince \u03a6and\u0398are bundle maps over \u03d5and\u03d5\u22121, respectively. Since X0is forward-invariant,\n(xn)n\u2208N0\u2286X0. By compactness of X0\u2229\u03c0\u22121(z), the sequence (xn)n\u2208N0has a convergent subnet with some\nlimit x\u2032\u2208\u03c0\u22121(z). Since X0has closed fibers, (\u0398k)\u22121(X0)\u2229\u03c0\u22121(z) = (\u0398k)\u22121(X0\u2229\u03c0\u22121(\u03d5\u2212k(z)))is closed.\nSince X0is forward-invariant and \u0398is a left-inverse of \u03a6, we have xn\u2208(\u0398k)\u22121(X0)for all k, n\u2208N0with\nk\u2264n. Hence, we must have x\u2032\u2208(\u0398k)\u22121(X0)\u2229\u03c0\u22121(z)for all k\u2208N0, that is, x\u2032\u2208X\u221e\u2229\u03c0\u22121(z).\nRecall that the omega limit set of a point x\u2208Xunder \u03a6is\u03c9\u03a6(x) =T\nn\u2208NS\nm\u2265n{\u03a6m(x)}. It is easy\nto see that \u03c9\u03a6(x)\u2286 S\u03a6\u2229\u03c0\u22121(\u03c9\u03d5(\u03c0(x))). We recall that a topological space is called first-countable if every\npoint admits a countable neighborhood basis. If Xis first-countable, then\n\u03c9\u03a6(x) ={x\u2032\u2208X:\u2203(nk)k\u2208N\u2286Nsuch that nk\u2192 \u221eand\u03a6nk(x)\u2192x\u2032}.\nProposition 4.13. Suppose Xis first-countable. Let x\u2208X\u221esuch that \u03c0(x)is eventually periodic under \u03d5.\nThen, \u03c9\u03a6(x)\u2286 S\u03a6\u2229X\u221e.\nProof.Letx\u2032\u2208\u03c9\u03a6(x), and take an unbounded sequence (nk)k\u2208N\u2286Nwith xk:= \u03a6nk(x)\u2192x\u2032. Since \u03c0(x)\nis eventually periodic under \u03d5, there exists a subsequence (xkl)lof(xk)ksuch that \u03c0(xkl) =\u03d5nkl(\u03c0(x))is\nconstant in l. We know from Lemma 4.11 that X\u221ehas closed fibers. Thus, x\u2032\u2208X\u221e.\n4.3 Stochastic dynamical systems\nNow, suppose XandZare completely regular and \u03a6admits a continuous left-inverse \u0398:X\u2192X. Let\nPX\u2286P(X)andPZ\u2286P(Z)satisfy \u03c0\u2217(PX)\u2286 PZand(\u0398\u2217)\u22121(PX) =PX. Then, consider the push-forward\n\u03a6\u2217:PX\u2192 P X, which has the left-inverse \u0398\u2217:PX\u2192 P X. We remark that these maps are well-defined since\nthe push-forward of a Radon measure under a continuous map is itself a Radon measure. The dynamical\nsystem \u03a6\u2217is \u2018stochastic\u2019 in the sense that it encodes the evolution of the law of a random variable under\niterations of \u03a6. However, as an abstract dynamical system, \u03a6\u2217is a deterministic function on the space of\nRadon probability measures. In particular, we can study its solution set, which we know equals its global\nattractor due to the existence of a left-inverse;\nS\u03a6\u2217={\u00b5\u2208 PX: \u03a6n\n\u2217\u0398n\n\u2217\u00b5=\u00b5for all n\u2208N}=\\\nn\u2208N\u03a6n\n\u2217(PX).\n7It is tempting to consider the dynamical system \u03a6\u2032= \u03a6|X\u221eon the state space X\u2032=X\u221e. Then, S\u03a6\u2032=S\u03a6\u2229X\u221eis closed\nin the topology of X\u2032. If\u03c0is proper, then \u03c0\u2032=\u03c0|X\u2032:X\u2032\u2192 Zhas compact fibers. But, in general, \u03c0\u2032is not closed.\n18\n\n--- Page 19 ---\nStochastic dynamics learning with state-space systems\nWe refer to elements in S\u03a6\u2217asstochastic solutions of the dynamical system \u03a6. The next proposition\njustifies this denomination. Indeed, the statement of Proposition 4.14 is equivalent to saying that an element\n\u00b5\u2208 PXis a solution of the dynamical system \u03a6\u2217if and only if some (in fact, every) X-valued random variable\nXwith law \u00b5satisfies \u03a6n(\u0398n(X)) =Xalmost surely for all n\u2208N. Recall that the support of a measure is\nthe set of all points of which every open neighborhood has strictly positive measure.\nProposition 4.14. An element \u00b5\u2208 PXbelongs to S\u03a6\u2217if and only if the support of \u00b5is contained in S\u03a6.\nProof.Suppose the support of \u00b5is contained in S\u03a6. Note that (\u03a6n\u25e6\u0398n)\u22121(A)\u2229S\u03a6=A\u2229S\u03a6for any n\u2208N\nand any measurable A\u2286X. Thus,\n\u03a6n\n\u2217\u0398n\n\u2217\u00b5(A) =\u00b5((\u03a6n\u25e6\u0398n)\u22121(A)) =\u00b5(\u03a6n\u25e6\u0398n)\u22121(A)\u2229 S\u03a6) =\u00b5(A\u2229 S\u03a6) =\u00b5(A),\nwhich shows that \u00b5\u2208 S\u03a6\u2217. Conversely, suppose \u00b5\u2208 S\u03a6\u2217, and let x\u2208X\\S\u03a6. Then, there exists some n\u2208N\nwith x /\u2208\u03a6n(X). Since \u0398is a continuous left-inverse of \u03a6, the latter is closed; see Lemma A.1. Thus, \u03a6n(X)is\na closed set. Since Xis completely regular, there exists an open neighborhood V\u2286Xofxwith V\u2229\u03a6n(X) =\u2205.\nEquivalently, (\u03a6n)\u22121(V) =\u2205. Now, for any \u00b5\u2208 S\u03a6\u2217, we have \u00b5(V) = \u03a6n\n\u2217\u0398n\n\u2217\u00b5(V) = \u0398n\n\u2217\u00b5(\u2205) = 0. Thus, x\ndoes not belong to the support of \u00b5.\nRemark 4.15. Suppose \u03c7:Z\u2192Xis a solution of \u03a6. Let (wt)t\u2208Z\u2286(0,1)satisfyP\nt\u2208Zwt= 1. Then, the\nmeasure \u00b5=P\nt\u2208Zwt\u03b4\u03c7(t)belongs to S\u03a6\u2217. In particular, there are elements in S\u03a6\u2217whose support belongs\nto the complement of the chain-recurrent set of \u03a6(if that complement is non-empty).\nIt is straight-forward to see that stochastic solutions can be obtained from pushing forward measures in\nPZunder a continuous map into S\u03a6. More precisely, if S:Z \u2192 S \u03a6is continuous, then S\u2217(PZ)\u2229 PX\u2286 S\u03a6\u2217.\nIn Proposition 4.16 below, recall that the set-valued fiber-map S\u03a6becomes a map Z \u2192Xunder the USP,\nand that continuity of S\u03a6is guaranteed by Proposition 4.5 if \u03c0is proper.\nProposition 4.16. Suppose \u03a6has the USP and S\u03a6is continuous. Then, (S\u03a6)\u2217(PZ)\u2229 PX=S\u03a6\u2217. In\nparticular, if (S\u03a6)\u2217(PZ)\u2286 PX, then \u03a6\u2217has the USP.\nProof.We observed above that (S\u03a6)\u2217(PZ)\u2229 PX\u2286 S\u03a6\u2217. For the reverse inclusion, let A\u2286Xbe measurable.\nBy the USP, A\u2229 S\u03a6= (S\u03a6\u25e6\u03c0)\u22121(A)\u2229 S\u03a6(here, S\u03a6denotes both the subset of Xand its fiber-map). This\nand Proposition 4.14 yield for any \u00b5\u2208 S\u03a6\u2217\n\u00b5(A) =\u00b5(A\u2229 S\u03a6) =\u00b5((S\u03a6\u25e6\u03c0)\u22121(A)\u2229 S\u03a6) =\u00b5((S\u03a6\u25e6\u03c0)\u22121(A)) = (S\u03a6)\u2217\u03c0\u2217\u00b5(A),\nwhich shows that S\u03a6\u2217\u2286(S\u03a6)\u2217(PZ).\nAs noted in Example 3.2, complete regularity of the underlying spaces ensures that push-forward maps\ninduced by continuous maps become continuous with respect to the weak topologies and that the weak\ntopology on the space of Radon probability measures is Hausdorff (as is any finer topology). Thus, if P(X)\nandP(Z)are endowed with the weak topologies, then \u03a6\u2217is a continuous bundle map over \u03d5\u2217with continuous\nleft-inverse \u0398\u2217. As such, the results that have been developed in Section 4.2 can also be applied to the\ndynamical system \u03a6\u2217. One may also work with topologies finer than the weak topologies such as Wasserstein\ntopologies if continuity of the various push-forward maps continues to hold.\nRemark 4.17. Stochastic solutions as defined in this paper are the laws of random variables that are\ndeterministic solutions almost surely. An extensive branch of literature has studied random dynamical\nsystems without passing to the level of the laws. For such systems, the notion of an attractor involves\na random set, that is, a family of sets parametrized by the underlying probability space of the source of\nrandomness [5,20]. The approach with such a random system can reveal more detailed behavior of the\ndynamics since it can study typical random trajectories. However, it generally requires stronger assumptions\n(such as compactness of the state space) and quickly runs into technical obstacles, which we avoid with the\npush-forward dynamical system. For our purposes, the information that we can infer on the level of the laws\nis sufficient.\n5. Proofs of results on state-space systems\nWe return to the notation from Sections 2 and 3 and prove all results stated in those sections, using the tools\ndeveloped in Section 4.\n19\n\n--- Page 20 ---\nOrtega and Rossmannek\n5.1 Proofs for deterministic solutions\nProof of Proposition 2.3. First, we show by induction that \u03c6n\u25e6Tnis the identity on \u03c4\u22121(Sdet)for all\nn\u2208N. For any (x, u)\u2208 X\u2212\u00d7 U, we have \u03c6\u25e6T(x, u) = (x, u)if and only if f(x\u22121, u0) =x0, which is indeed\nthe case for any (x, u)\u2208\u03c4\u22121(Sdet). Ifn\u22652and(x, u)\u2208\u03c4\u22121(Sdet), then\n\u03c6n\u25e6Tn(x, u) =\u03c6\u25e6\u03c6n\u22121\u25e6Tn\u22121(T(x, u)) =\u03c6(T(x, u)) = ( x, u),\nwhere we applied the induction hypothesis to T(x, u), which is possible since T(Sdet)\u2286 Sdet, and then the\ninduction start to (x, u). Thus, \u03c4\u22121(Sdet)\u2286 S\u03c6. Since \u03c4(U) =U\u2212, we also have Sdet=\u03c4(\u03c4\u22121(Sdet))\u2286\u03c4(S\u03c6).\nIt remains to be shown that \u03c4(S\u03c6)\u2286 Sdet, which would also imply S\u03c6\u2286\u03c4\u22121(\u03c4(S\u03c6))\u2286\u03c4\u22121(Sdet). Denote\np0:X\u2212\u00d7 U\u2192 X,(x, u)7\u2192x0, and let (x, u)\u2208 S\u03c6. We need to show that f(xt\u22121, ut) =xtfor all t\u2208Z\u2212.\nFort= 0, this is immediate from applying p0on both sides of the equality \u03c6\u25e6T(x, u) = (x, u). For t\u2264 \u22121,\nnote that S\u03c6is forward-invariant under Tand, hence, T\u2212t\u22121(x, u)\u2208 S\u03c6. Then,\nf(xt\u22121, ut) =p0\u25e6\u03c6\u25e6T(T\u2212t\u22121(x, u)) =p0(T\u2212t\u22121(x, u)) =xt,\nwhich concludes the proof.\nProof of Theorem 2.6. Since the topology on X\u2212\u00d7 U\u2212is at least as fine as the product topology, it\nis clear that Sdetis closed. Since X\u2212is compact, the projection \u03c0U\u2212:X\u2212\u00d7 U\u2212\u2192 U\u2212is proper. By\nProposition 4.5, Sdetis upper hemi-continuous. As the composition of an upper hemi-continuous map and\na continuous map, Odet=H(Sdet)is also upper hemi-continuous. If the ESP holds (in the outputs), then\nSdet(Odet) is singleton-valued and upper hemi-continuity agrees with usual continuity.\nProof of Proposition 2.13. Let us verify that Proposition 4.9 is applicable with X=X\u221e=X\u2212\u00d7 U,\nZ=U,Z\u2212=U\u2212,Y=Y\u2212\u00d7 U\u2212,\u03c0=\u03c0U,\u03d5=\u03c6,\u03c4=\u03c4, and \u03b7=H\u25e6\u03c4. With these choices,\n\u03b7(S\u03d5\u2229X\u221e\u2229(\u03c4\u25e6\u03c0)\u22121(u)) =H(\u03c4(S\u03c6))\u2229\u03c0\u22121\nU\u2212(u) =Odet(u)\nforany u\u2208 U\u2212. Hence, ifwefindthatthethreehypothesesofProposition4.9aresatisfied, thentheconclusion\nof Proposition 4.9 is exactly the desired statement. Hypothesis (i): Suppose (x, u)and(x\u2032, u\u2032)are elements\nofS\u03c6such that \u03b7(T(x, u)) = \u03b7(T(x\u2032, u\u2032))and\u03c0(x, u) =\u03c0(x\u2032, u\u2032). Then, u=u\u2032andh(x\u22121) =h(x\u2032\n\u22121). By\nRemark 2.11, we have h(x0) =h(x\u2032\n0)and, hence, \u03b7(x, u) =\u03b7(x\u2032, u\u2032), which verifies hypothesis (i). Hypothesis\n(ii): It is evident from Proposition 2.3 that \u03c4(S\u03c6\u2229\u03c0\u22121(u))depends on uonly through \u03c4(u). Hypothesis (iii):\nThis is precisely stipulated in Assumption 2.1.(iv) (and recall that \u03c4mapsUsurjectively onto U\u2212).\nProposition 2.9 corresponds to the special case of Proposition 2.13 in which the readout is the identity.\nProof of Theorem 2.15. As in the proof of Theorem 2.6, we can apply Proposition 4.5 to the set-valued\nmapsSdetandOdet=H(Sdet). Thus, we obtain that D(Sdet)andD(Odet)contain a residual set. The final\nstatement follows from Propositions 2.9 and 2.13.\n5.2 Proofs for general stochastic solutions\nFrom now on, let j+:X\u2212\u00d7 U\u2212\u2192 X\u2212\u00d7 Udenote a continuous right-inverse of the truncation, whose\nexistence we stipulated in Assumption 2.1.(ii).\nProof of Proposition 3.3. It is clear that (iii) \u21d2(ii)\u21d2(i). If (i) holds, then \u00b5(Sdet) = 1. This is sufficient\nto deduce (iii) because Sdetis closed and X\u2212\u00d7 U\u2212is completely regular.\nNext, we know from Proposition 4.14 that an element \u00b5\u2208 Pstatebelongs to S\u03c6\u2217if and only if the support\nof\u00b5is contained in S\u03c6. The support of \u03c4\u2217\u00b5is contained in the closure of \u03c4(support (\u00b5)). In particular, the\nsupport of an element in \u03c4\u2217(S\u03c6\u2217)is contained in \u03c4(S\u03c6) =Sdet. This and the equivalence between (i) and\n(iii) show \u03c4\u2217(S\u03c6\u2217)\u2286 Sstoch. Conversely, suppose \u00b5\u2208 Sstoch. Then, the support of \u00b5is contained in Sdet, and\nthe support of j+\u00b5is contained in the closure of j+(Sdet). Since j+(Sdet)is a subset of \u03c4\u22121(Sdet) =S\u03c6, we\nconclude that \u00b5=\u03c4\u2217j+\n\u2217\u00b5\u2208\u03c4\u2217(S\u03c6\u2217).\n20\n\n--- Page 21 ---\nStochastic dynamics learning with state-space systems\nProof of Proposition 3.7. We know from Proposition 4.16 that (S\u03c6)\u2217(Pin)\u2229 Pstate=S\u03c6\u2217. Observe that\n\u03c4\u2217((S\u03c6)\u2217(Pin)\u2229 Pstate) = ( \u03c4\u25e6 S\u03c6)\u2217(Pin)\u2229 P\u2212\nstate. Furthermore, the equality \u03c4\u25e6 S\u03c6=Sdet\u25e6\u03c4implies\n(\u03c4\u25e6 S\u03c6)\u2217(Pin) =Sdet\n\u2217(P\u2212\nin). Thus,\nSstoch=\u03c4\u2217(S\u03c6\u2217) =\u03c4\u2217((S\u03c6)\u2217(Pin)\u2229 Pstate) = (\u03c4\u25e6 S\u03c6)\u2217(Pin)\u2229 P\u2212\nstate=Sdet\n\u2217(P\u2212\nin)\u2229 P\u2212\nstate.\nLemma 5.1. The set Sstochis closed (in the subspace topology of P\u2212\nstate).\nProof.Note that \u03c6n\u25e6Tn\u25e6j+\u25e6\u03c4=j+\u25e6\u03c4\u25e6\u03c6n\u25e6Tnfor all n\u2208N. This observation implies j+\n\u2217(Sstoch)\u2286 S\u03c6\u2217.\nNow, suppose \u00b5\u2208 P\u2212\nstateis a cluster point of a net (\u00b5i)i\u2208I\u2286 Sstoch. Then, j+\n\u2217\u00b5\u2208 Pstateis a cluster point of\n(j+\n\u2217\u00b5i)i\u2208I\u2286 S\u03c6\u2217in the weak topology. Since S\u03c6\u2217is weakly closed (as a subset of Pstate), it contains j+\n\u2217\u00b5. In\nparticular, \u00b5=\u03c4\u2217j+\n\u2217\u00b5\u2208\u03c4\u2217(S\u03c6\u2217) =Sstoch.\nProof of Theorem 3.8. SinceSstochis a closed subset of P\u2212\nstateby Lemma 5.1, its fiber map is upper hemi-\ncontinuous by Proposition 4.5. The same goes for Ostoch=H\u2217(Sstoch), being the composition of an upper\nhemi-continuous map and a continuous map. If the stochastic ESP holds, then these are singleton-valued\nmaps and upper hemi-continuity agrees with usual continuity. Without the stochastic ESP but if P\u2212\ninand\nP\u2212\nstateare Polish, the second part of Proposition 4.5 applies.\nTo prove the remaining results, we need to have a closer look at causal solutions. Before we continue with\nstochastic state-space systems, we briefly recap the definition and most important properties of conditional\nindependence of sigma-algebras, which we used to define causal measures.\n5.3 Interlude: conditional independence\nConsider a probability space (\u2126,A,P)and sub-sigma-algebras \u03a31,\u03a32,\u03a33\u2286 A. We remind the reader that\n\u03a31and\u03a33are said to be conditionally independent given \u03a32(written \u03a31\u22a5 \u22a5\u03a32\u03a33) with respect to Pif\nEP[\u03b21\u03b23|\u03a32] =EP[\u03b21|\u03a32]EP[\u03b23|\u03a32]P-a.s. (5.1)\nfor any two bounded functions \u03b21, \u03b23:\u2126\u2192Rthat are \u03a31-measurable and \u03a33-measurable, respectively. An\nequivalent characterization is\nEP[\u03b23|\u03a31\u2228\u03a32] =EP[\u03b23|\u03a32]P-a.s. (5.2)\nfor any bounded \u03a33-measurable function \u03b23:\u2126\u2192R. Here, \u03a31\u2228\u03a32denotes the sigma-algebra generated\nby\u03a31\u222a\u03a32. Various properties of conditional independence can be found, for example, in [43, Chapter 8]\nand [18, Chapter 7.3]. The following facts will be useful.\nRemark 5.2. Consider sub-sigma-algebras \u03a31,\u03a3\u2032\n1,\u03a32,\u03a3\u2032\n2,\u03a33\u2286 A.\n(i)If\u03a3\u2032\n1\u2286\u03a31\u2228\u03a32and if \u03a31\u22a5 \u22a5\u03a32\u03a33wrtP, then \u03a3\u2032\n1\u22a5 \u22a5\u03a32\u03a33wrtP. This is immediate from the equivalent\ncharacterization (5.2) and the tower property of conditional expectations.\n(ii)If\u03a3\u2032\n3\u2286\u03a33and if \u03a31\u22a5 \u22a5\u03a32\u03a33wrtP, then \u03a31\u22a5 \u22a5\u03a32\u2228\u03a3\u2032\n3\u03a33wrtP. Indeed, it is immediate from the\ndefinition (5.1)that \u03a33\u22a5 \u22a5\u03a32\u03a31and\u03a3\u2032\n3\u22a5 \u22a5\u03a32\u03a31. Then, applying the equivalent characterization (5.2)\nto each immediately yields \u03a31\u22a5 \u22a5\u03a32\u2228\u03a3\u2032\n3\u03a33.\nFurthermore, let (\u2126\u2032,A\u2032,P\u2032)be another probability space and G: (\u2126\u2032,A\u2032)\u2192(\u2126,A)be measurable.\n(iii)It holds \u03a31\u22a5 \u22a5\u03a32\u03a33wrtG\u2217P\u2032if and only if G\u22121(\u03a31)\u22a5 \u22a5G\u22121(\u03a32)G\u22121(\u03a33)wrtP\u2032. This follows from how\nconditional expectations behave under push-forward transformations, namely\nEG\u2217P\u2032[\u03b2|\u03a3]\u25e6 G=EP\u2032\u0002\n\u03b2\u25e6 G |G\u22121(\u03a3)\u0003\nP\u2032-a.s. (5.3)\nfor any bounded A-measurable function \u03b2: \u2126\u2192Rand any sub-sigma-algebra \u03a3\u2286 A.\nThe following lemma is a version of [43, Proposition 8.20]. For the sake of completeness, we provide a\nproof in Appendix A.\n21\n\n--- Page 22 ---\nOrtega and Rossmannek\nLemma 5.3 (Adaptation of Proposition 8.20 in [43]) .Let\u039bdenote the Lebesgue measure on [0,1]. For each\ni= 1,2,3, suppose \u03a3iis the sigma-algebra generated by a measurable map Vi:\u2126\u2192 V iinto a measurable\nspaceVi. Suppose g:V2\u00d7[0,1]\u2192 V 3is a measurable map that satisfies (\u03c0V2\u00d7g)\u2217((V2)\u2217P\u2297\u039b) = ( V2\u00d7V3)\u2217P.\nLetG:V1\u00d7 V2\u00d7[0,1]\u2192 V 1\u00d7 V2\u00d7 V3denote the map G(v1, v2, \u03bb) = (v1, v2, g(v2, \u03bb)). Then, the following\nare equivalent.\n(i)\u03a31and\u03a33are conditionally independent given \u03a32wrtP.\n(ii)The map Gsatisfies G\u2217((V1\u00d7V2)\u2217P\u2297\u039b) = ( V1\u00d7V2\u00d7V3)\u2217P.\nRemark 5.4. In Lemma 5.3, such a measurable map gexists as soon as V3is a standard Borel space; see,\nfor example, [43, Theorem 8.17] and its proof.\nThe following is an immediate consequence of Lemma 5.3 and Remark 5.4.\nLemma 5.5. For each i= 1,2,3, suppose \u03a3iis the sigma-algebra generated by a measurable map Vi:\u2126\u2192 V i\ninto a measurable space Vi, with V3a standard Borel space. Furthermore, suppose the map V1\u00d7V2\u00d7V3:\u2126\u2192\nV1\u00d7V2\u00d7V3admits a measurable left-inverse. Let Pdenote the set of all probability measures P\u2032on(\u2126,A)with\nrespect to which \u03a31\u22a5 \u22a5\u03a32\u03a33and which satisfy (V2\u00d7V3)\u2217P\u2032= (V2\u00d7V3)\u2217P. Then, the map P\u20327\u2192(V1\u00d7V2)\u2217P\u2032\nis injective on P.\nIn general, for fixed sigma-algebras, the set of measures with respect to which conditional independence\nholds is closed neither in the weak topology nor in a Wasserstein topology [53]. Only in the topology of\ntotal variation does this set become closed [17,54]. However, this topology is too strong for our purposes.\nThe issue in the weak and Wasserstein topologies is that information in \u03a32could degenerate in a limit, and\n\u03a31and\u03a33become dependent once the information from \u03a32is lost. Conversely, if we stipulate that only\ninformation in \u03a31varies, then conditional independence remains preserved under weak limits. This is the\nmessage of the next lemma.\nLemma 5.6. For each i= 1,2,3, suppose \u03a3iis the sigma-algebra generated by a continuous map Vi:\u2126\u2192 V i\ninto a Polish space Vi. Suppose there exists a sequence (Pn)n\u2208Nof probability measures on \u2126with the following\nproperties.\n(i)\u03a31\u22a5 \u22a5\u03a32\u03a33wrtPnfor all n\u2208N.\n(ii)(V2\u00d7V3)\u2217Pn= (V2\u00d7V3)\u2217Pfor all n\u2208N.\n(iii)(V1\u00d7V2\u00d7V3)\u2217Pis an accumulation point of ((V1\u00d7V2\u00d7V3)\u2217Pn)n\u2208Nin the weak topology.\nThen, \u03a31\u22a5 \u22a5\u03a32\u03a33wrtP.\nProof.By hypotheses (i) and (ii), we know from Lemma 5.3 and Remark 5.4 that G\u2217((V1\u00d7V2)\u2217Pn\u2297\u039b) =\n(V1\u00d7V2\u00d7V3)\u2217Pnfor all n\u2208N. We need to argue that this equality continues to hold with Pin place of\nPndespite the possibility that gmay not be continuous. But this is indeed not a problem since V1andV2\nare Polish, Gis continuous in its V1-coordinate, and (\u03c0V2\u00d7\u03c0[0,1])\u2217(V1\u00d7V2)\u2217Pn\u2297\u039b) = ( V2)\u2217P\u2297\u039bdoes not\ndepend on n; see [38, Corollary 2.9] and also [74, Example 2].\n5.4 Proofs for causal stochastic solutions\nTo prove results about causal solutions, we first translate the notion of causality to measures on P(X\u2212\u00d7 U).\nLet\u03a3\u2212\nXand\u03a3Ube the sigma-algebras on X\u2212\u00d7 Ugenerated by the projections onto X\u2212andU, respectively,\nand let \u03a3\u2212\nUbe the sigma-algebra on X\u2212\u00d7 Ugenerated by the composition X\u2212\u00d7 U\u2192 U\u2212of the projection\nand the truncation.\nDefinition 5.7. We call a measure \u00b5\u2208P(X\u2212\u00d7 U)causal at time t\u2208Z\u2212if\u03a3\u2212\nXand\u03a3Uare conditionally\nindependent given \u03a3\u2212\nUwith respect to T\u2212t\n\u2217\u00b5. We call \u00b5causalif it is causal at all times t\u2208Z\u2212.\nIt is immediate from the definition that if \u00b5\u2208P(X\u2212\u00d7 U)is causal, then so is T\u2217\u00b5. In particular, if \u03c6\u2217\u00b5\nis causal, then so is T\u2217\u03c6\u2217\u00b5=\u00b5. We will see in Lemma 5.9.(i) below that the converse also holds.\nRemark 5.8. Note that (T\u2212t)\u22121(\u03a3\u2212\nX) =\u03c4\u22121(\u03a3X,t)and(T\u2212t)\u22121(\u03a3\u2212\nU) =\u03c4\u22121(\u03a3U,t). It follows from Re-\nmark 5.2.(iii) that a measure \u00b5\u2208P(X\u2212\u00d7U)is causal at time t\u2208Z\u2212if and only if \u03c4\u22121(\u03a3X,t)\u22a5 \u22a5\u03c4\u22121(\u03a3U,t)\u03a3U\nwrt\u00b5. Using that \u03c4\u22121(\u03a3U,0)\u2286\u03a3Uand applying Remark 5.2.(iii) once more, we deduce that if \u00b5is causal,\nthen so is \u03c4\u2217\u00b5in the sense of Definition 3.11.\n22\n\n--- Page 23 ---\nStochastic dynamics learning with state-space systems\nLetX0\u2286 P statedenote the set of all measures that are causal at time zero. The next lemma asserts that\nthis set satisfies the assumptions that we posed on the set X0at the beginning of Section 4.2.3.\nLemma 5.9. The set X0has the following properties.\n(i)It is forward-invariant under \u03c6\u2217.\n(ii)Its fiber X0\u2229(\u03c0U)\u22121\n\u2217(\u039e)is weakly closed (in the subspace topology of Pstate) for any \u039e\u2208P(U).\n(iii)For any \u039e\u2208 P inand\u00b5\u2212\u2208 P\u2212\nstate\u2229(\u03c0U\u2212)\u22121\n\u2217(\u03c4\u2217\u039e), there exists a unique \u00b5\u2208X0\u2229(\u03c0U)\u22121\n\u2217(\u039e)with\n\u03c4\u2217\u00b5=\u00b5\u2212.\nProof.(i) Let \u00b5\u2208X0. We need to show that \u03a3\u2212\nX\u22a5 \u22a5\u03a3\u2212\nU\u03a3Uwrt\u03c6\u2217\u00b5. By Remark 5.2.(iii) and since\n\u03c6\u22121(\u03a3U) = \u03a3 U, it suffices to convince ourselves that \u03c6\u22121(\u03a3\u2212\nX)\u22a5 \u22a5\u03c6\u22121(\u03a3\u2212\nU)\u03a3Uwrt\u00b5. Since we work with\nproduct topologies (by Assumption 3.10), the sigma-algebra \u03c6\u22121(\u03a3\u2212\nU)can be written as \u03a3\u2212\nU\u2228\u03a31\nU, where \u03a31\nUis\nthe sigma-algebra generated by the projection X\u2212\u00d7U\u2192 U,(x, u)7\u2192u1. Thus, we can apply Remark 5.2.(ii)\nand the premise that \u00b5\u2208X0to find that \u03a3\u2212\nX\u22a5 \u22a5\u03c6\u22121(\u03a3\u2212\nU)\u03a3Uwrt\u00b5. Then, we apply Remark 5.2.(i) with\n\u03c6\u22121(\u03a3\u2212\nX)\u2286\u03a3\u2212\nX\u2228\u03a3\u2212\nU\u2228\u03a31\nUto conclude that \u03c6\u22121(\u03a3\u2212\nX)\u22a5 \u22a5\u03c6\u22121(\u03a3\u2212\nU)\u03a3Uwrt\u00b5as desired.\n(ii) This follows from Lemma 5.6 (since X\u2212andUare Polish by Assumption 3.10, the weak topology on\nPstateis metrizable and, hence, it suffices to consider sequences instead of nets).\n(iii) Take a measurable map g:U\u2212\u00d7[0,1]\u2192 Uthat satisfies (\u03c0U\u2212\u00d7g)\u2217(\u03c4\u2217\u039e\u2297\u039b) = ( \u03c4\u00d7idU)\u2217\u039e, whose\nexistence we recalled in Remark 5.4. In particular, \u03c4\u25e6g=\u03c0U\u2212holds \u03c4\u2217\u039e\u2297\u039b-a.s., since U\u2212is Polish. Let\n\u00b5:= (\u03c0X\u2212\u00d7(g\u25e6\u03c0U\u2212\u00d7[0,1]))\u2217(\u00b5\u2212\u2297\u039b). Then, (\u03c0X\u2212\u00d7(\u03c0U\u2212\u25e6\u03c4))\u2217\u00b5=\u00b5\u2212and((\u03c0U\u2212\u25e6\u03c4)\u00d7\u03c0U)\u2217\u00b5= (\u03c4\u00d7idU)\u2217\u039e. An\napplication of Lemma 5.3 yields \u03a3\u2212\nX\u22a5 \u22a5\u03a3\u2212\nU\u03a3Uwrt\u00b5. Thus, \u00b5\u2208X0. Uniqueness follows from Lemma 5.5.\nAs in Section 4.2.3, let X\u221e:=T\nn\u2208N0(Tn\n\u2217)\u22121(X0). It is straight-forward to see that X\u221eis exactly the set\nof causal measures in Pstate. Recall that \u03c4\u2217(S\u03c6\u2217)\u2229Pcausal(X\u2212\u00d7 U\u2212) =Sc,stoch, by definition.\nLemma 5.10. We have \u03c4\u2217(S\u03c6\u2217\u2229X\u221e) =Sc,stoch.\nProof.The inclusion \u03c4\u2217(S\u03c6\u2217\u2229X\u221e)\u2286 Sc,stochis immediate from Remark 5.8. Conversely, suppose we are\ngiven an element \u00b5\u2208 Sc,stoch. We show that j+\n\u2217\u00b5belongs to S\u03c6\u2217\u2229X\u221e. We had already seen in the proof\nof Lemma 5.1 that j+\n\u2217(Sstoch)\u2286 S\u03c6\u2217. It remains to be shown that j+\n\u2217\u00b5is causal. Since \u00b5is causal and\n(j+)\u22121(\u03a3U)\u2286\u03a3U,0, we know that \u03a3X,t\u22a5 \u22a5\u03a3U,t(j+)\u22121(\u03a3U)wrt\u00b5for all t\u2208Z\u2212. By Remark 5.2.(iii) and the\nfact that (j+)\u22121(\u03c4\u22121(\u03a3U,t)) = \u03a3 U,tand(j+)\u22121(\u03c4\u22121(\u03a3X,t)) = \u03a3 X,t, we find that \u03c4\u22121(\u03a3X,t)\u22a5 \u22a5\u03c4\u22121(\u03a3U,t)\u03a3U\nwrtj+\n\u2217\u00b5. This is equivalent to causality of j+\n\u2217\u00b5as noted in Remark 5.8.\nAs our last technical ingredient, we show that measures that are causal at time zero and whose truncation\nagrees with a causal measure are themselves causal.\nLemma 5.11. Let\u00b5\u2208X\u221eand\u00b5\u2032\u2208X0satisfy \u03c4\u2217\u00b5=\u03c4\u2217\u00b5\u2032. Then, \u00b5\u2032\u2208X\u221e. Furthermore, if \u00b5\u2208 S\u03c6\u2217, then\n\u00b5\u2032\u2208 S\u03c6\u2217.\nProof.Fixt\u2208Z\u2212. We need to show that (T\u2212t)\u2217\u00b5\u2032\u2208X0. Abbreviate V1:=T\u2212t\u25e6\u03c0X\u2212\u25e6\u03c4,V2:=T\u2212t\u25e6\u03c0U\u2212\u25e6\u03c4,\nV:=\u03c0U\u2212\u25e6\u03c4, and V3:=\u03c0U. Let g\u2212:U\u2212\u00d7[0,1]\u2192 U\u2212andg+:U\u2212\u00d7[0,1]\u2192 Ube measurable maps such that\n(\u03c0U\u2212\u00d7g\u2212)\u2217((V2)\u2217\u00b5\u2032\u2297\u039b) = ( V2\u00d7V)\u2217\u00b5\u2032and(\u03c0U\u2212\u00d7g+)\u2217(V\u2217\u00b5\u2032\u2297\u039b) = ( V\u00d7V3)\u2217\u00b5\u2032, respectively, whose existence\nwe recalled in Remark 5.4 ( U\u2212andUare Polish by Assumption 3.10). In particular, T\u2212t\u25e6g\u2212=\u03c0U\u2212holds\n(V2)\u2217\u00b5\u2032\u2297\u039b-a.s., since U\u2212is Polish. Let \u03b3= (\u03b31, \u03b32):[0,1]\u2192[0,1]\u00d7[0,1]be a measurable map such that\n\u03b3\u2217\u039b = \u039b \u2297\u039b. Then, let g:U\u2212\u00d7[0,1]\u2192 Ube given by g(u, \u03bb) =g+(g\u2212(u, \u03b31(\u03bb)), \u03b32(\u03bb)). This map satisfies\n(\u03c0U\u2212\u00d7g)\u2217((V2)\u2217\u00b5\u2032\u2297\u039b) = ( V2\u00d7V3)\u2217\u00b5\u2032. By Remark 5.8, we need to show that \u03c4\u22121(\u03a3X,t)\u22a5 \u22a5\u03c4\u22121(\u03a3U,t)\u03a3U\nwrt\u00b5\u2032. By the same remark, the hypotheses, and the fact that \u03c4\u22121(\u03a3X,t)\u2286\u03c4\u22121(\u03a3X,0), we know that\n\u03c4\u22121(\u03a3X,t)\u22a5 \u22a5\u03c4\u22121(\u03a3U,t)\u03c4\u22121(\u03a3U,0)and\u03c4\u22121(\u03a3X,t)\u22a5 \u22a5\u03c4\u22121(\u03a3U,0)\u03a3Uwrt\u00b5\u2032. Let G\u2212:X\u2212\u00d7 U\u2212\u00d7[0,1]\u2192\nX\u2212\u00d7U\u2212\u00d7[0,1]andG+:X\u2212\u00d7U\u2212\u00d7[0,1]\u2192 X\u2212\u00d7U\u2212\u00d7Ube the maps G\u2212(x, u, \u03bb) = (x, g\u2212(u, \u03b31(\u03bb)), \u03b32(\u03bb))\nandG+(x, u, \u03bb) = (x, u, g+(u, \u03bb)), respectively. ByLemma5.3, wehave G\u2212\n\u2217((V1\u00d7V2)\u2217\u00b5\u2032\u2297\u039b) = ( V1\u00d7V)\u2217\u00b5\u2032\u2297\u039b\nandG+\n\u2217((V1\u00d7V)\u2217\u00b5\u2032\u2297\u039b) = ( V1\u00d7V\u00d7V3)\u2217\u00b5\u2032. In particular, the map G\u2032:= (\u03c0X\u2212\u00d7(T\u2212t\u25e6\u03c0U\u2212)\u00d7\u03c0U)\u25e6G+\u25e6G\u2212\nsatisfies G\u2032\n\u2217((V1\u00d7V2)\u2217\u00b5\u2032\u2297\u039b) = ( V1\u00d7V2\u00d7V3)\u2217\u00b5\u2032. Let G:X\u2212\u00d7 U\u2212\u00d7[0,1]\u2192 X\u2212\u00d7 U\u2212\u00d7 Ube the map\nG(x, u, \u03bb) = (x, u, g(u, \u03bb)). Then, G=G\u2032holds (V1\u00d7V2)\u2217\u00b5\u2032\u2297\u039b-a.s., since T\u2212t\u25e6g\u2212=\u03c0U\u2212holds (V2)\u2217\u00b5\u2032\u2297\u039b-\na.s. Applying Lemma 5.3 once more, we have found that indeed \u03c4\u22121(\u03a3X,t)\u22a5 \u22a5\u03c4\u22121(\u03a3U,t)\u03a3Uwrt\u00b5\u2032.\n23\n\n--- Page 24 ---\nOrtega and Rossmannek\nNow, suppose \u00b5\u2208 S\u03c6\u2217. To see that \u00b5\u2032\u2208 S\u03c6\u2217, fixn\u2208Nand consider \u00b5n:=\u03c6n\n\u2217Tn\n\u2217\u00b5\u2032. By Lemma 4.11,\n\u00b5n\u2208X0. Note that \u03c4\u25e6\u03c6n\u25e6Tn=\u03c4\u25e6\u03c6n\u25e6Tn\u25e6j+\u25e6\u03c4. Thus,\n\u03c4\u2217\u00b5n=\u03c4\u2217\u03c6n\n\u2217Tn\n\u2217\u00b5\u2032=\u03c4\u2217\u03c6n\n\u2217Tn\n\u2217j+\n\u2217\u03c4\u2217\u00b5\u2032=\u03c4\u2217\u03c6n\n\u2217Tn\n\u2217j+\n\u2217\u03c4\u2217\u00b5=\u03c4\u2217\u03c6n\n\u2217Tn\n\u2217\u00b5=\u03c4\u2217\u00b5=\u03c4\u2217\u00b5\u2032.\nAt the same time, (\u03c0U)\u2217\u00b5n= (\u03c0U)\u2217\u00b5\u2032since \u03c0U\u25e6\u03c6n\u25e6Tn=\u03c0U. By the uniqueness in Lemma 5.9.(iii), we\nmust have \u00b5n=\u00b5\u2032.\nProof of Proposition 3.13. By Lemma 4.11, the restriction \u03d5:=\u03c6\u2217|X\u221eis an autonomous dynamical\nsystem on X\u221eand a bundle map over \u03c3\u2217. Note that S\u03d5=S\u03c6\u2217\u2229X\u221e. We show that the causal stochastic\nESP of the state-space system transfers to the USP of \u03d5. Let \u039e\u2208 Pin. Then, there is a unique \u00b5\u2212\u2208 Sc,stoch\nwith (\u03c0U\u2212)\u2217\u00b5\u2212=\u03c4\u2217\u039e. Take an element \u00b5\u2208 S\u03c6\u2217\u2229X\u221ewith \u03c4\u2217\u00b5=\u00b5\u2212. By Lemma 5.9.(iii), there is a\nunique \u00b5\u2032\u2208X0with \u03c4\u2217\u00b5\u2032=\u00b5\u2212and(\u03c0U)\u2217\u00b5\u2032= \u039e. By Lemma 5.11, \u00b5\u2032\u2208 S\u03d5. This concludes the USP\nof\u03d5. By Corollary 4.4, S\u03d5=Ssync\n\u03d5. Now, suppose \u039e\u2212\u2208 P\u2212\ninis periodic under T\u2217with minimal period\nn\u2208N. Let \u00b5\u2212\u2208 Sc,stochbe the unique causal stochastic solution for the input \u039e\u2212. By an application of the\nKolmogorov extension theorem, there exists some \u039e\u2208P(U)that is periodic under \u03c3\u2217with minimal period\nnand that satisfies \u03c4\u2217\u039e = \u039e\u2212; this is where we use the hypothesis U=\u03c4\u22121(U\u2212)\u2013 see Lemma A.2 in the\nappendix for details. Let \u00b5\u2208 S\u03d5be the unique element with (\u03c0U)\u2217\u00b5= \u039e, which we know from above satisfies\n\u03c4\u2217\u00b5=\u00b5\u2212. By Proposition 4.3, \u00b5\u2208Sync-Per(\u03d5, \u03c3\u2217). Thus, \u03d5n(\u00b5) =\u00b5, and nis minimal. In particular,\nTn\n\u2217\u00b5\u2212=\u03c4\u2217Tn\n\u2217\u00b5=\u03c4\u2217Tn\n\u2217\u03d5n(\u00b5) =\u03c4\u2217\u00b5=\u00b5\u2212.\nProof of Proposition 3.14. We claim that the restriction of (\u03c0U)\u2217toX0has weakly compact fibers.\nOnce this is shown, Proposition 4.12 implies that S\u03c6\u2217\u2229X\u221ehas non-empty fibers (note that the fibers in\nLemma5.9.(ii)areweaklyclosed), andthenLemma5.10concludestheproof. Let \u039e\u2208 Pin. ByLemma5.9.(iii),\nthere exists a bijection \u03b1:P\u2212\nstate\u2229(\u03c0U\u2212)\u22121\n\u2217(\u03c4\u2217\u039e)\u2192X0\u2229(\u03c0U)\u22121\n\u2217(\u039e). The domain of \u03b1is compact by the\nhypothesis. As in the proof of Lemma 5.6, the map \u03b1is continuous with respect to the weak topologies.\nThus, the image of \u03b1is weakly compact.\nNext, we apply Proposition 4.9 to the dynamical system \u03c6\u2217to deduce Proposition 3.20. As in the\ndeterministic case, Proposition 3.18 is recovered by taking the readout to be the identity.\nProof of Proposition 3.20. Let us verify that Proposition 4.9 is applicable with X=Pstate,X\u221e=X\u221e,\nZ=Pin,Z\u2212=P\u2212\nin,Y=P\u2212\nout,\u03c0= (\u03c0U)\u2217,\u03d5=\u03c6\u2217,\u03c4=\u03c4\u2217, and \u03b7= (H\u25e6\u03c4)\u2217. With these choices, by\nLemma 5.10,\n\u03b7(S\u03d5\u2229X\u221e\u2229(\u03c4\u25e6\u03c0)\u22121(\u039e)) = H\u2217(\u03c4\u2217(S\u03c6\u2217)\u2229Pcausal(X\u2212\u00d7 U\u2212))\u2229\u03c0\u22121\nU\u2212(\u039e) = Oc,stoch(\u039e)\nforany \u039e\u2208 P\u2212\nin. Hence, ifwefindthatthethreehypothesesofProposition4.9aresatisfied, thentheconclusion\nof Proposition 4.9 is exactly the desired statement. Hypothesis (i): Let \u00b5, \u00b5\u2032\u2208 S\u03c6\u2217\u2229X\u221esatisfy H\u2217\u03c4\u2217T\u2217\u00b5=\nH\u2217\u03c4\u2217T\u2217\u00b5\u2032and(\u03c0U)\u2217\u00b5= (\u03c0U)\u2217\u00b5\u2032. We need to show that H\u2217\u03c4\u2217\u00b5=H\u2217\u03c4\u2217\u00b5\u2032. Let H\u2032:X\u2212\u00d7 U\u2192 Y\u2212\u00d7 Ube\nthe extension of the readout determined by \u03c4\u25e6H\u2032=H\u25e6\u03c4and\u03c0U\u25e6H\u2032=\u03c0U. It is evident from this and\nRemark 5.2 that H\u2032\n\u2217(X0)is a subset of the analogously defined Y0\u2286P(Y\u2212\u00d7U). By the relations determining\nH\u2032and the hypotheses on \u00b5and\u00b5\u2032, we have \u03c4\u2217H\u2032\n\u2217T\u2217\u00b5=\u03c4\u2217H\u2032\n\u2217T\u2217\u00b5\u2032and(\u03c0U)\u2217H\u2032\n\u2217T\u2217\u00b5= (\u03c0U)\u2217H\u2032\n\u2217T\u2217\u00b5\u2032.\nLemma 5.5 yields H\u2032\n\u2217T\u2217\u00b5=H\u2032\n\u2217T\u2217\u00b5\u2032. Let g:Y \u00d7 U\u2212\u2192 Ybe a measurable extension of the measurable\nmap promised by the assumption that the system measurably distinguishes reachable states. Consider\nthe map G:Y\u2212\u00d7 U\u2192 Y\u2212\u00d7 U\u2212given by G(y, u) = (( y, g(y0, \u03c3(u))),(\u03c4\u25e6\u03c3)(u)). This map satisfies\nG \u25e6H\u2032\u25e6T(x, u) =H\u25e6\u03c4(x, u)for all (x, u)\u2208 S\u03c6. This and Proposition 4.14 yield\nH\u2217\u03c4\u2217\u00b5=G\u2217H\u2032\n\u2217T\u2217\u00b5=G\u2217H\u2032\n\u2217T\u2217\u00b5\u2032=H\u2217\u03c4\u2217\u00b5\u2032.\nHypothesis (ii): Let \u039e,\u039e\u2032\u2208 Pinsatisfy \u03c4\u2217\u039e =\u03c4\u2217\u039e\u2032, and suppose \u00b5\u2208 S\u03c6\u2217\u2229X\u221e\u2229(\u03c0U)\u22121\n\u2217(\u039e\u2032). We need to show\nthat \u03c4\u2217\u00b5\u2208\u03c4\u2217(S\u03c6\u2217\u2229X\u221e\u2229(\u03c0U)\u22121\n\u2217(\u039e)). By Lemma 5.9.(iii), there exists a unique \u00b5\u2032\u2208X0with \u03c4\u2217\u00b5\u2032=\u03c4\u2217\u00b5\nand(\u03c0U)\u2217\u00b5\u2032= \u039e. By Lemma 5.11, we have \u00b5\u2032\u2208 S\u03c6\u2217\u2229X\u221e, as desired. Hypothesis (iii): This is stipulated in\nAssumption 3.16 (recall that \u03c4\u2217(Pin) =P\u2212\nin).\n24\n\n--- Page 25 ---\nStochastic dynamics learning with state-space systems\n6. Conclusion\nWe have developed a comprehensive theoretical framework for understanding fading memory and solution\nstability in state-space systems with both deterministic and stochastic inputs. Our results show that,\ngenerically, these systems exhibit fading memory even in the absence of classical ESP conditions, thus\nexplaining the success of RC models that do not satisfy standard contractivity requirements.\nInthestochasticsetting, weproposedanewdistributionalnotionofasolutionbasedonattractordynamics\non spaces of probability measures. This approach naturally recovers the notion of a solution defined through\nalmost sure equality of the state equation, and it leads to a coherent theory that captures essential features\none expects from stochastic solutions. We extend several results on the ESP and fading memory from the\ndeterministic to the stochastic setting, and rigorously discuss differences that arise such as the intricacies of\nprobabilistic causality.\nOur work builds on and extends the abstract dynamical systems perspective initiated in earlier founda-\ntional studies, enabling a unified treatment that is both rigorous and broadly applicable. Key concepts such\nas fading memory and causal structures appear naturally and can be embedded into other context where\nthey appear naturally such as stochastic filtering (see Appendix B).\nWe considered stability of solutions as functions of the input. Future work may discuss robustness of\nthe model \u2013 both deterministically and stochastically. When an unknown system is learned with a reservoir\ncomputing model trained on data, the learned model is a perturbation of the unknown model in function\nspace. Understanding the robustness of such a perturbation is a crucial aspect of mathematical learning\ntheory.\nAcknowledgments\nThe authors acknowledge partial financial support from the School of Physical and Mathematical Sciences\nof the Nanyang Technological University. The second author is funded by an Eric and Wendy Schmidt AI\nin Science Postdoctoral Fellowship at the Nanyang Technological University. The authors thank Lyudmila\nGrigoryeva, G. Manjunath, and Michael Greinecker for helpful comments and discussions.\nAppendix A. Lemmas from topology and measure theory\nA map is proper if it is closed and has compact fibers, and it is quasi-proper if preimages of compact sets are\ncompact. The following result is mostly standard in topology [12, p. 97ff]. We review a proof for the part\nthat is not standard.\nLemma A.1. LetXandYbe Hausdorff spaces and f:X \u2192 Ybe continuous. Then, the following are\nequivalent.\n(i)The map fis proper.\n(ii)The map fis closed and quasi-proper.\n(iii)For any net (xi)i\u2208I\u2286 Xand any cluster point y\u2208 Yof the net (f(xi))i\u2208Ithere exists a cluster point\nx\u2208 Xof the net (xi)i\u2208Iwithf(x) =y.\nUsing the equivalence to (iii), it becomes clear that if fadmits a continuous left-inverse, then fis proper.\nProof.It is straight-forward to see that (iii) \u21d2(ii)\u21d2(i). We show that (i) \u21d2(iii). For any i\u2208I, letAibe\nthe closure of the set {xj:j\u2265i}so that y\u2208f(Ai). Since fis closed, f(Ai)=f(Ai). Thus, Ai\u2229f\u22121(y)is\nnon-empty. These sets have the finite intersection property. Indeed, given finitely many i1, . . . , i N\u2208I, there\nexists some i\u2208Iwith in\u2264ifor all 1\u2264n\u2264N, and then Ai1\u2229 \u00b7\u00b7\u00b7 \u2229 AiN\u2229f\u22121(y)contains the non-empty\nsetAi\u2229f\u22121(y). Compactness of f\u22121(y)implies thatT\ni\u2208IAi\u2229f\u22121(y)is non-empty. Take an element x\nfrom that intersection. If xwas not a cluster point of (xi)i\u2208I, then there existed some i\u2208Iand an open set\nU\u2286 Xwith x\u2208Usuch that xj/\u2208Ufor all j\u2265i. But then Ai\u2229Uwere empty, contradicting x\u2208Ai\u2229U.\nIn the next lemma, we flesh out an application of the Kolmogorov extension theorem that we needed in\nthe proof of Proposition 3.13. Below, U\u2212and\u03c4\u22121(U\u2212)are endowed with the product topologies.\nLemma A.2. LetUbe a Suslin space and U\u2212\u2286 UZ\u2212be Borel measurable. Denote the truncation UZ\u2192 UZ\u2212\nby\u03c4. Suppose \u039e\u2212\u2208P(U\u2212)is periodic under shifts. Then, there exists some \u039e\u2208P(\u03c4\u22121(U\u2212))that is periodic\nunder shifts with the same minimal period as \u039e\u2212and satisfies \u03c4\u2217\u039e = \u039e\u2212.\n25\n\n--- Page 26 ---\nOrtega and Rossmannek\nProof.LetTdenotetheright-shiftoperatoron UZ\u2212andon UZ. Let n\u2208Nbetheminimalperiodof \u039e\u2212under\nT\u2217. Given any finite subset \u039b\u2286Z, letp\u039b:UZ\u2192 U|\u039b|,u7\u2192(ut)t\u2208\u039bandp\u2212\n\u039b:U\u2212\u2192 U|\u039b|,u7\u2192(ut\u2212t\u039b)t\u2208\u039b,\nwhere t\u039b:=supt\u2208\u039b|nt|. Given any two nested finite subsets \u039b\u2032\u2286\u039b\u2286Z, letk\u039b\u2032,\u039b:= (t\u039b\u2212t\u039b\u2032)/n\u2208N0and\np\u039b\u2032,\u039b:U|\u039b|\u2192 U|\u039b\u2032|,(ut)t\u2208\u039b7\u2192(ut)t\u2208\u039b\u2032. Then, p\u039b\u2032,\u039b\u25e6p\u2212\n\u039b=p\u2212\n\u039b\u2032\u25e6Tnk\u039b\u2032,\u039b. This shows that the measures\n\u039e\u039b:= (p\u2212\n\u039b)\u2217\u039e\u2212satisfy the consistency condition of the Kolmogorov extension theorem [10, Corollary 7.7.2].\nBy that theorem, there exists a measure \u039e\u2032\u2208P(UZ)that satisfies (p\u039b)\u2217\u039e\u2032= \u039e \u039bfor all finite subsets \u039b\u2286Z.\nLetusshowthat Tn\n\u2217\u039e\u2032= \u039e\u2032. SinceweworkwiththeproducttopologyofaSuslinspace, itsufficestoshowthat\n(p\u039bk)\u2217Tn\n\u2217\u039e\u2032= (p\u039bk)\u2217\u039e\u2032for all k\u2208N, where \u039bk:={\u2212k, . . . , k }. Fix k\u2208N. Let \u039b\u2032:={\u2212k\u2212n, . . . , k \u2212n}and\n\u039b := \u039b k\u222a\u039b\u2032. Note that p\u039bk\u25e6Tn=p\u039b\u2032andp\u2212\n\u039b\u2032=p\u2212\n\u039bk\u25e6Tn. Therefore, (p\u039bk)\u2217Tn\n\u2217\u039e\u2032= \u039e \u039b\u2032= \u039e \u039bk= (p\u039bk)\u2217\u039e\u2032,\nas desired. A similar argument shows that \u03c4\u2217\u039e\u2032= \u039e\u2212. In particular, the period of \u039e\u2032under T\u2217cannot be\nsmaller than n. Finally, let \u039ebe the restriction of \u039e\u2032to\u03c4\u22121(U\u2212), which is a probability measure since\n\u039e\u2032(\u03c4\u22121(U\u2212)) =\u03c4\u2217\u039e\u2032(U\u2212) = \u039e\u2212(U\u2212) = 1.\nFor the sake of completeness, we provide a proof of Lemma 5.3.\nProof of Lemma 5.3. This lemma is an adaptation of [43, Proposition 8.20]. Let us spell out the details.\nTo ensure the existence of an independent uniform random variable, consider the product probability space\n(\u2126\u2032,A\u2032,P\u2032) := (\u2126 \u00d7[0,1],A \u2228 B ([0,1]),P\u2297\u039b)and the projection \u03d1:\u2126\u2032\u2192[0,1], where B([0,1])is the Borel\nsigma-algebra on [0,1]. Let \u03a3\u2032\ni\u2286 A\u2032be the sigma-algebra generated by the extended maps V\u2032\ni:\u2126\u2032\u2192 V i,\n(\u03c9, \u03bb)7\u2192Vi(\u03c9),i= 1,2,3. Note that \u03a31\u22a5 \u22a5\u03a32\u03a33wrtPif and only if \u03a3\u2032\n1\u22a5 \u22a5\u03a3\u2032\n2\u03a3\u2032\n3wrtP\u2032. Since \u03d1is\nindependent of (V\u2032\n1, V\u2032\n2), we trivially have \u03a3\u2032\n1\u22a5 \u22a5\u03a3\u2032\n2B([0,1])wrtP\u2032. By Remark 5.2.(i), it also holds that\n\u03a3\u2032\n1\u22a5 \u22a5\u03a3\u2032\n2\u03a3\u2032\n2\u2228 B([0,1])wrtP\u2032. Thus, by (5.2),\nEP\u2032[\u03b2(g(V\u2032\n2, \u03d1))|\u03a3\u2032\n1\u2228\u03a3\u2032\n2] =EP\u2032[\u03b2(g(V\u2032\n2, \u03d1))|\u03a3\u2032\n2]P\u2032-a.s. (A.1)\nfor any bounded measurable function \u03b2:V3\u2192R. Now, suppose (ii) holds. Given any bounded \u03a3\u2032\n3-measurable\nfunction \u03b2\u2032\n3:\u2126\u2032\u2192R, take a measurable function \u03b2:V3\u2192Rsuch that \u03b2\u2032\n3=\u03b2\u25e6V\u2032\n3. Two applications of (5.3)\nwith the map V\u2032\n1\u00d7V\u2032\n2\u00d7V\u2032\n3, the hypothesis (ii), and (A.1) yield\nEP\u2032[\u03b2\u2032\n3|\u03a3\u2032\n1\u2228\u03a3\u2032\n2] =EP\u2032[\u03b2(g(V\u2032\n2, \u03d1))|\u03a3\u2032\n1\u2228\u03a3\u2032\n2] =EP\u2032[\u03b2(g(V\u2032\n2, \u03d1))|\u03a3\u2032\n2] =EP\u2032[\u03b2\u2032\n3|\u03a3\u2032\n2]P\u2032-a.s.\nThis shows that \u03a3\u2032\n1\u22a5 \u22a5\u03a3\u2032\n2\u03a3\u2032\n3wrtP\u2032. Conversely, suppose (i) holds. Let \u02c6\u03a3ibe the sigma-algebra on V1\u00d7V2\u00d7V3\ngenerated by the projection onto Vi. Let \u03b2:V3\u2192Rbe bounded and measurable. By (5.3), we have\nE(V1\u00d7V2\u00d7V3)\u2217Ph\n\u03b2\u25e6\u03c0V3|\u02c6\u03a31\u2228\u02c6\u03a32i\n\u25e6(V1\u00d7V2\u00d7V3) =EP\u2032[\u03b2\u25e6V\u2032\n3|\u03a3\u2032\n1\u2228\u03a3\u2032\n2]P\u2032-a.s.\nThis, in turn, equals EP\u2032[\u03b2\u25e6V\u2032\n3|\u03a3\u2032\n2]P\u2032-a.s. by (i). The hypothesis on the function gyields\nEP\u2032[\u03b2\u25e6V\u2032\n3|\u03a3\u2032\n2] =EP\u2032[\u03b2(g(V\u2032\n2, \u03d1))|\u03a3\u2032\n2]P\u2032-a.s.\nBy another application of (5.3) and (A.1), we find that\nEP\u2032[\u03b2(g(V\u2032\n2, \u03d1))|\u03a3\u2032\n2] =EG\u2217((V1\u00d7V2)\u2217P\u2297\u039b)h\n\u03b2\u25e6\u03c0V3|\u02c6\u03a31\u2228\u02c6\u03a32i\n\u25e6GP\u2032-a.s.\nWe have shown that\nE(V1\u00d7V2\u00d7V3)\u2217Ph\n\u03b2\u25e6\u03c0V3|\u02c6\u03a31\u2228\u02c6\u03a32i\n\u25e6(V1\u00d7V2\u00d7V3) =EG\u2217((V1\u00d7V2)\u2217P\u2297\u039b)h\n\u03b2\u25e6\u03c0V3|\u02c6\u03a31\u2228\u02c6\u03a32i\n\u25e6GP\u2032-a.s.\nfor any bounded measurable \u03b2:V3\u2192R. The tower property of conditional expectation yields\nE(V1\u00d7V2\u00d7V3)\u2217P[\u03b21\u03b22\u03b23] =EG\u2217((V1\u00d7V2)\u2217P\u2297\u039b)[\u03b21\u03b22\u03b23]\nfor any bounded \u02c6\u03a3i-measurable \u03b2i:V1\u00d7 V2\u00d7 V3\u2192R,i= 1,2,3. We conclude the lemma.\nAppendix B. Statistical inference\nConsider a triple of stochastic processes (Y, X, U) = ( Yt, Xt, Ut)t\u2208Z\u2212with values in Y\u2212\u00d7 X\u2212\u00d7 U\u2212such\nthat the joint law of (X, U)is a stochastic solution of the state-space system and the joint law of (Y, U)a\nstochastic output. Throughout the paper, we worked with outputs of the form Yt=h(Xt)because this was\n26\n\n--- Page 27 ---\nStochastic dynamics learning with state-space systems\nsufficient to capture the echo state and fading memory properties we were interested in. In practice, one\noften models the output by Yt=h(Xt, Vt)with an additional measurement noise Vt. In (Bayesian) filtering,\none is concerned with sampling from the conditional law of the hidden state X0given the observations Y.\nThe linear Gaussian case is well-understood, that is, when the inputs Utand the measurement noise Vt\nare iid Gaussian random variables and fandhare linear transformations. This classical filtering problem\nis solved by the Kalman filter, pioneered by the eponymous Kalman [44]. More general cases are typically\ntackled with the likes of the extended or unscented Kalman filters, which first linearize the transformations\nand approximate the inputs and measurement noise by iid Gaussians and then apply the classical Kalman\nfilter [75]; or with particle filters and their various extensions, which pass carefully sampled \u2018particles\u2019 through\nthe non-linear transformations and weigh them appropriately [45]. Crucially, the theoretical analyses of all\nthese filtering methods assume the hidden states to be Markovian. This is guaranteed if the inputs are\nmutually independent but fails in general if the inputs exhibit statistical interdependence.\nConsider the more general case in which the inputs are not mutually independent but Uitself is Markovian.\nSince past states typically depend on past inputs and the latter may exhibit interdependence with the current\ninput, we cannot hope that the states Xare Markovian. However, the sequence of augmented states\nX\u2032\nt:= (Xt, Ut)is Markovian if there is no direct interdependence between Utand past states, that is, X\u2032\nis Markovian if the joint law of (X, U)is causal. This highlights the need to understand the appearance of\ncausal structures in stochastic solutions of state-space systems. Although we expect real-world systems to\nbe causal, theoretical justifications are lacking. The states of a (random or noisy) real-world system, whose\nevolution is governed by a state map and which has long been evolving in the past, represent a stochastic\nsolution in the forward attractor of the system, more precisely, in one of its omega limit sets. If the underlying\ninputs of the system (random factors, noise, controls) are periodic, then we know from Proposition 4.13 that\nthe system is indeed causal. It is an interesting open problem to extend this result to non-periodic inputs.\nAugmenting the states by the inputs and, going further, augmenting them by (finitely many) past\nstates and past inputs is a known trick in filtering. In particular, this has been done in [57] to obtain\nan approximately Markovian model out of a non-Markovian one. That the augmented states are indeed\napproximately Markovian relies on a fading memory assumption posed in that work. Fading memory also\nappears as ground for some convergence results for filtering algorithms [45]. In light of such results being\nenabled by fading memory assumptions, it is a critical theoretical contribution that we established generic\nfading memory of general state-space systems in Theorems 2.6 and 3.8.\nReferences\n[1]Allam, A., Feuerriegel, S., Rebhan, M., and Krauthammer, M. Analyzing Patient Trajectories With\nArtificial Intelligence. J Med Internet Res 23 , 12 (Dec 2021), e29812.\n[2]Amig\u00f3, J. M., Dale, R., King, J. C., and Lehnertz, K. Generalized synchronization in the presence\nof dynamical noise and its detection via recurrent neural networks. Chaos: An Interdisciplinary Journal of\nNonlinear Science 34 , 12 (Dec 2024), 123156.\n[3]Appeltant, L., Soriano, M. C., Van der Sande, G., Danckaert, J., Massar, S., Dambre, J.,\nSchrauwen, B., Mirasso, C. R., and Fischer, I. Information processing using a single dynamical node as\ncomplex system. Nature Communications 2 , 1 (2011), 468.\n[4]Arcomano, T., Szunyogh, I., Wikner, A., Pathak, J., Hunt, B. R., and Ott, E. A hybrid approach\nto atmospheric modeling that combines machine learning with a physics-based numerical model. Journal of\nAdvances in Modeling Earth Systems 14 , 3 (2022), e2021MS002712.\n[5]Arnold, L. Random Dynamical Systems , 1 ed. Springer Monographs in Mathematics. Springer Berlin, Heidel-\nberg, 1998.\n[6]Aubin, J.-P., and Frankowska, H. Set-Valued Analysis , 1 ed., vol. 234 of Modern Birkh\u00e4user Classics .\nBirkh\u00e4user Boston, 2009.\n[7]Babin, A. V., and Pilyugin, S. Y. Continuous dependence of attractors on the shape of domain. Journal\nof Mathematical Sciences 87 , 2 (Nov 1997), 3304\u20133310. transl. from Zap. Nauchn. Sem. POMI, 221 (1995), p.\n58\u201366.\n[8]Ballarin, G., Dellaportas, P., Grigoryeva, L., Hirt, M., van Huellen, S., and Ortega, J.-P. Reser-\nvoir computing for macroeconomic forecasting with mixed-frequency data. International Journal of Forecasting\n40, 3 (2024), 1206\u20131237.\n[9]Berry, T., and Das, S. Learning Theory for Dynamical Systems. SIAM Journal on Applied Dynamical\nSystems 22 , 3 (2023), 2082\u20132122.\n[10]Bogachev, V. I. Measure Theory , 1 ed. Springer-Verlag, Berlin Heidelberg, 2007.\n27\n\n--- Page 28 ---\nOrtega and Rossmannek\n[11]Bogmans, C., Gomez-Gonzalez, P., Ganpurev, G., Melina, G., Pescatori, A., and Thube, S. Power\nHungry: How AI Will Drive Energy Demand. IMF Working Papers 2025 , 81 (4 2025), 32.\n[12]Bourbaki, N. General Topology , 1 ed. Springer Berlin, Heidelberg, 1998.\n[13]Boyd, S., and Chua, L. Fading memory and the problem of approximating nonlinear operators with Volterra\nseries.IEEE Transactions on Circuits and Systems 32 , 11 (1985), 1150\u20131161.\n[14]Buehler, H., Horvath, B., Lyons, T., Perez Arribas, I., and Wood, B. Generating Financial Markets\nWith Signatures. SSRN:3657366 (2020).\n[15]Buehner, M., and Young, P. A tighter bound for the echo state property. IEEE Transactions on Neural\nNetworks 17 , 3 (2006), 820\u2013824.\n[16]Ceni, A., Ashwin, P., Livi, L., and Postlethwaite, C. The echo index and multistability in input-driven\nrecurrent neural networks. Physica D: Nonlinear Phenomena 412 (2020), 132609.\n[17]Cheridito, P., and Eckstein, S. Optimal transport and Wasserstein distances for causal models.\narXiv:2303.14085v2 (2023).\n[18]Chow, Y. S., and Teicher, H. Probability Theory: Independence, Interchangeability, Martingales , 3 ed.\nSpringer Texts in Statistics. Springer New York, 1997.\n[19]Chua, L., and Green, D. A qualitative analysis of the behavior of dynamic nonlinear networks: Steady-state\nsolutions of nonautonomous networks. IEEE Transactions on Circuits and Systems 23 , 9 (1976), 530\u2013550.\n[20]Crauel, H., and Flandoli, F. Attractors for random dynamical systems. Probability Theory and Related\nFields 100 , 3 (Sep 1994), 365\u2013393.\n[21]Desheng, L., and Kloeden, P. E. Equi-attraction and the continuous dependence of attractors on parameters.\nGlasgow Mathematical Journal 46 , 1 (2004), 131\u2013141.\n[22]Engelking, R. General Topology , vol. 6 of Sigma Series in Pure Mathematics . Heldermann Verlag Berlin, 1989.\n[23]Eroglu, D., Lamb, J. S. W., and Pereira, T. Synchronisation of chaos and its applications. Contemporary\nPhysics 58 , 3 (2017), 207\u2013243.\n[24]Fernando, C., and Sojakka, S. Pattern Recognition in a Bucket. In Advances in Artificial Life (2003),\nW. Banzhaf, J. Ziegler, T. Christaller, P. Dittrich, and J. T. Kim, Eds., Springer Berlin Heidelberg, pp. 588\u2013597.\n[25]Francq, C., and Zakoian, J.-M. GARCH models , 2 ed. John Wiley & Sons, 2019.\n[26]Gonon, L., Grigoryeva, L., and Ortega, J.-P. Approximation bounds for random neural networks and\nreservoir systems. The Annals of Applied Probability 33 , 1 (2023), 28\u201369.\n[27]Gonon, L., and Ortega, J.-P. Fading memory echo state networks are universal. Neural Networks 138 (2021),\n10\u201313.\n[28]Graves, A. Generating Sequences With Recurrent Neural Networks. arXiv:1308.0850v5 (2014).\n[29]Grigoryeva, L., Hart, A. G., and Ortega, J.-P. Chaos on compact manifolds: Differentiable synchro-\nnizations beyond the Takens theorem. Physical Review E - Statistical Physics, Plasmas, Fluids, and Related\nInterdisciplinary Topics 103 (2021), 062204.\n[30]Grigoryeva, L., Hart, A. G., and Ortega, J.-P. Learning strange attractors with reservoir systems.\nNonlinearity 36 (2023), 4674\u20134708.\n[31]Grigoryeva, L., and Ortega, J.-P. Echo state networks are universal. Neural Networks 108 (2018), 495\u2013508.\n[32]Grigoryeva, L., and Ortega, J.-P. Universal discrete-time reservoir computers with stochastic inputs and\nlinear readouts using non-homogeneous state-affine systems. Journal of Machine Learning Research 19 , 24 (2018),\n1\u201340.\n[33]Grigoryeva, L., and Ortega, J.-P. Differentiable reservoir computing. Journal of Machine Learning Research\n20, 179 (2019), 1\u201362.\n[34]Grigoryeva, L., and Ortega, J.-P. Dimension reduction in recurrent networks by canonicalization. Journal\nof Geometric Mechanics 13 , 4 (2021), 647\u2013677.\n[35]Gu, A., and Dao, T. Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv:2312.00752v2\n(2023).\n[36]Hewamalage, H., Bergmeir, C., and Bandara, K. Recurrent Neural Networks for Time Series Forecasting:\nCurrent status and future directions. International Journal of Forecasting 37 , 1 (2021), 388\u2013427.\n[37]Hoang, L. T., Olson, E. J., and Robinson, J. C. On the continuity of global attractors. Proceedings of the\nAmerican Mathematical Society 143 , 10 (2015), 4389\u20134395.\n[38]Jacod, J., and Memin, J. Sur un type de convergence intermediaire entre la convergence en loi et la convergence\nen probabilite. In S\u00e9minaire de Probabilit\u00e9s XV 1979/80 (1981), J. Az\u00e9ma and M. Yor, Eds., vol. 850 of Lecture\nNotes in Mathematics , Springer Berlin Heidelberg, pp. 529\u2013546.\n[39]Jaeger, H. The \u201cecho state\u201d approach to analysing and training recurrent neural networks \u2013 with an Erratum\nnote. Tech. Rep. GMD Report 148, German National Research Center for Information Technology, 2010.\n28\n",
  "project_dir": "artifacts/projects/enhanced_stat.ML_2508.07876v1_Stochastic_dynamics_learning_with_state_space_syst",
  "communication_dir": "artifacts/projects/enhanced_stat.ML_2508.07876v1_Stochastic_dynamics_learning_with_state_space_syst/.agent_comm",
  "assigned_at": "2025-08-12T19:27:07.575636",
  "status": "assigned"
}